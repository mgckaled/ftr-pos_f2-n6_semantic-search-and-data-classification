<!--markdownlint-disable-->

# Bloco C: Classificação de Dados com Inteligência Artificial

## Índice

- [Bloco C: Classificação de Dados com Inteligência Artificial](#bloco-c-classificação-de-dados-com-inteligência-artificial)
  - [Índice](#índice)
  - [Resumo Executivo](#resumo-executivo)
  - [1. Introdução à Classificação com IA](#1-introdução-à-classificação-com-ia)
    - [1.1 Contexto Histórico](#11-contexto-histórico)
    - [1.2 Classificação Clássica vs. Baseada em IA](#12-classificação-clássica-vs-baseada-em-ia)
    - [1.3 Modelos Fundacionais](#13-modelos-fundacionais)
  - [2. Fundamentos Teóricos de Classificação](#2-fundamentos-teóricos-de-classificação)
    - [2.1 Formulação Matemática](#21-formulação-matemática)
    - [2.2 Taxonomia de Problemas](#22-taxonomia-de-problemas)
    - [2.3 Desafios Contemporâneos](#23-desafios-contemporâneos)
  - [3. Métricas de Avaliação](#3-métricas-de-avaliação)
    - [3.1 Matriz de Confusão](#31-matriz-de-confusão)
    - [3.2 Métricas Fundamentais](#32-métricas-fundamentais)
    - [3.3 Métricas Avançadas](#33-métricas-avançadas)
    - [3.4 Considerações sobre Classes Desbalanceadas](#34-considerações-sobre-classes-desbalanceadas)
  - [4. Abordagem 1: Embeddings Pré-treinados + KNN](#4-abordagem-1-embeddings-pré-treinados--knn)
    - [4.1 Arquitetura e Fundamentos](#41-arquitetura-e-fundamentos)
    - [4.2 Implementação Prática](#42-implementação-prática)
    - [4.3 Vantagens e Limitações](#43-vantagens-e-limitações)
  - [5. Abordagem 2: Fine-tuning de Modelos Pré-treinados](#5-abordagem-2-fine-tuning-de-modelos-pré-treinados)
    - [5.1 Transfer Learning e Fine-tuning](#51-transfer-learning-e-fine-tuning)
    - [5.2 Arquitetura: DistilBERT](#52-arquitetura-distilbert)
    - [5.3 Processo de Treinamento](#53-processo-de-treinamento)
    - [5.4 Estratégias de Otimização](#54-estratégias-de-otimização)
  - [6. Abordagem 3: Classificação com LLMs](#6-abordagem-3-classificação-com-llms)
    - [6.1 Prompt Engineering para Classificação](#61-prompt-engineering-para-classificação)
    - [6.2 Zero-shot vs. Few-shot Learning](#62-zero-shot-vs-few-shot-learning)
    - [6.3 Structured Output e Validação](#63-structured-output-e-validação)
    - [6.4 Considerações de Custo e Latência](#64-considerações-de-custo-e-latência)
  - [7. Análise Comparativa das Abordagens](#7-análise-comparativa-das-abordagens)
    - [7.1 Comparação de Performance](#71-comparação-de-performance)
    - [7.2 Análise de Trade-offs](#72-análise-de-trade-offs)
    - [7.3 Critérios de Seleção](#73-critérios-de-seleção)
  - [8. Casos de Uso e Aplicações](#8-casos-de-uso-e-aplicações)
    - [8.1 Análise de Sentimento](#81-análise-de-sentimento)
    - [8.2 Classificação de Documentos](#82-classificação-de-documentos)
    - [8.3 Detecção de Spam e Conteúdo Tóxico](#83-detecção-de-spam-e-conteúdo-tóxico)
    - [8.4 Aplicações Especializadas](#84-aplicações-especializadas)
  - [9. Conclusões e Perspectivas Futuras](#9-conclusões-e-perspectivas-futuras)
  - [Apêndices](#apêndices)
    - [Apêndice A: Formulações Matemáticas](#apêndice-a-formulações-matemáticas)
    - [Apêndice B: Exemplos de Código](#apêndice-b-exemplos-de-código)
    - [Apêndice C: Comparação Detalhada das Abordagens](#apêndice-c-comparação-detalhada-das-abordagens)
    - [Apêndice D: Datasets de Benchmark](#apêndice-d-datasets-de-benchmark)
    - [Apêndice E: Glossário](#apêndice-e-glossário)
    - [Apêndice F: Perguntas Frequentes](#apêndice-f-perguntas-frequentes)
  - [Referências](#referências)

---

## Resumo Executivo

Este documento apresenta uma análise sistemática e abrangente das abordagens contemporâneas para classificação de dados textuais utilizando técnicas de inteligência artificial. A classificação textual, tarefa fundamental em processamento de linguagem natural, evoluiu substancialmente nas últimas décadas, transitando de métodos estatísticos tradicionais para arquiteturas neurais profundas baseadas em modelos fundacionais.

O documento está estruturado em nove seções principais que cobrem aspectos teóricos, metodológicos e práticos da classificação com IA. Inicia-se com contextualização histórica que contrasta abordagens clássicas (Naive Bayes, SVM, Regressão Logística) com métodos modernos baseados em transformers e modelos de linguagem de grande escala. A segunda seção estabelece fundamentos matemáticos e taxonômicos necessários para compreensão rigorosa do problema. A terceira seção dedica-se inteiramente às métricas de avaliação, incluindo precisão, revocação, F1-score, ROC-AUC e análise de matriz de confusão.

As seções 4, 5 e 6 constituem o núcleo do documento, apresentando três abordagens distintas para classificação textual: (1) Embeddings pré-treinados combinados com K-Nearest Neighbors, representando abordagem de baixo custo e rápida implementação; (2) Fine-tuning de modelos transformer pré-treinados (DistilBERT), equilibrando performance e eficiência; (3) Prompting de Large Language Models (LLMs) como Gemini, explorando capacidades zero-shot e few-shot.

A seção 7 oferece análise comparativa sistemática das três abordagens considerando dimensões de acurácia, latência, custo computacional, interpretabilidade e facilidade de implementação. A seção 8 explora casos de uso práticos em domínios variados. Finalmente, a seção 9 sintetiza conclusões e discute perspectivas futuras.

Os apêndices complementam o texto principal com formulações matemáticas rigorosas, implementações completas em Python, comparações tabulares detalhadas, descrições de datasets de benchmark e glossário técnico. Este material destina-se a estudantes avançados, pesquisadores e profissionais que buscam domínio tanto teórico quanto prático das técnicas estado-da-arte em classificação textual com IA.

---

## 1. Introdução à Classificação com IA

### 1.1 Contexto Histórico

A classificação de texto representa uma das tarefas mais antigas e fundamentais em processamento de linguagem natural (NLP). Historicamente, abordagens baseavam-se em features manuais e algoritmos de aprendizado de máquina clássicos. A era pré-deep learning caracterizava-se por:

**Representações Bag-of-Words (BoW)**: Documentos representados como vetores esparsos de frequências de palavras, desconsiderando ordem e contexto. Técnicas incluíam TF-IDF (Term Frequency-Inverse Document Frequency) para ponderação de importância de termos.

**Algoritmos Clássicos**: Naive Bayes, aproveitando independência condicional para classificação probabilística; Support Vector Machines (SVM), buscando hiperplanos de separação ótima em espaços de alta dimensionalidade; Regressão Logística, modelando probabilidades mediante funções sigmóides.

**Engenharia de Features**: Sucesso dependia criticamente de expertise em criação manual de features: n-gramas, POS tags, entidades nomeadas, features sintáticas e lexicais específicas do domínio.

A revolução deep learning, iniciada com word embeddings (Word2Vec, GloVe) e consolidada com arquiteturas transformer (BERT, GPT), transformou radicalmente o panorama. Modelos neurais aprendem representações distribuídas automaticamente, eliminando necessidade de engenharia manual de features e alcançando performance sem precedentes.

### 1.2 Classificação Clássica vs. Baseada em IA

**Abordagens Clássicas**:

*Características*:
- Representações esparsas de alta dimensionalidade
- Features manuais específicas do domínio
- Algoritmos com fundamentos estatísticos bem compreendidos
- Treinamento rápido em datasets moderados
- Interpretabilidade superior (especialmente Naive Bayes, Regressão Logística)

*Limitações*:
- Incapacidade de capturar contexto e semântica profunda
- Performance degrada significativamente com vocabulário não observado
- Requer expertise substancial em engenharia de features
- Dificuldade em modelar interações complexas entre features

**Abordagens Baseadas em IA (Deep Learning)**:

*Características*:
- Representações densas de dimensionalidade reduzida
- Aprendizado automático de features hierárquicas
- Captura de contexto e semântica mediante atenção e recorrência
- Transfer learning através de modelos pré-treinados
- Performance estado-da-arte em benchmarks

*Limitações*:
- Requer datasets massivos para treinamento from-scratch
- Custo computacional elevado (GPUs/TPUs necessários)
- Interpretabilidade reduzida (modelos "caixa-preta")
- Vulnerabilidade a adversarial examples
- Dependência de hiperparâmetros e arquitetura

### 1.3 Modelos Fundacionais

Modelos fundacionais (foundation models) representam paradigma emergente caracterizado por modelos de grande escala treinados em vastos corpora heterogêneos, servindo como base para adaptação a tarefas específicas mediante fine-tuning ou prompting.

**Características Distintivas**:

1. **Escala**: Bilhões de parâmetros (GPT-3: 175B, PaLM: 540B, GPT-4: estimado >1T)
2. **Treinamento Não-Supervisionado**: Objetivos self-supervised (masked language modeling, next token prediction)
3. **Emergência**: Capacidades não explicitamente treinadas emergem com escala (reasoning, few-shot learning)
4. **Adaptabilidade**: Transfer learning eficiente para domínios e tarefas variadas

**Modelos Relevantes para Classificação**:

- **BERT** (Bidirectional Encoder Representations from Transformers): Encoder bidirecional, excelente para tarefas de compreensão
- **DistilBERT**: Versão destilada do BERT (40% menor, 60% mais rápido, 97% da performance)
- **RoBERTa**: BERT otimizado com modificações no pré-treinamento
- **GPT-3/4**: Modelos generativos, excelentes para zero/few-shot via prompting
- **T5**: Framework unificado text-to-text
- **Gemini**: Modelo multimodal de última geração (Google DeepMind)

O paradigma fundacional democratizou acesso a modelos poderosos, permitindo que praticantes alcancem resultados próximos ao estado-da-arte sem recursos computacionais massivos mediante fine-tuning ou prompting.

---

## 2. Fundamentos Teóricos de Classificação

### 2.1 Formulação Matemática

Classificação de texto consiste em aprender função $f: \mathcal{X} \rightarrow \mathcal{Y}$ que mapeia espaço de entrada $\mathcal{X}$ (documentos textuais) para espaço de saída $\mathcal{Y}$ (conjunto de classes).

**Formulação Probabilística**:

Dado documento $x$ e conjunto de classes $C = \{c_1, c_2, ..., c_k\}$, busca-se estimar distribuição de probabilidade condicional $P(c|x)$. A classificação ótima sob risco 0-1 é:

$$\hat{c} = \arg\max_{c \in C} P(c|x)$$

**Aprendizado Supervisionado**:

Dado dataset de treinamento $D = \{(x_i, y_i)\}_{i=1}^N$ onde $x_i \in \mathcal{X}$ e $y_i \in \mathcal{Y}$, objetivo é minimizar risco empírico:

$$R_{emp}(f) = \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(x_i), y_i)$$

onde $\mathcal{L}$ é função de perda (e.g., cross-entropy para classificação).

**Representação Neural**:

Modelos neurais implementam $f$ como composição de transformações:

$$f(x) = f_L(f_{L-1}(...f_1(x; \theta_1); \theta_{L-1}); \theta_L)$$

onde cada $f_l$ representa camada neural parametrizada por $\theta_l$.

### 2.2 Taxonomia de Problemas

**Classificação Binária**: $|\mathcal{Y}| = 2$. Exemplos: detecção de spam, análise de sentimento positivo/negativo.

**Classificação Multiclasse**: $|\mathcal{Y}| > 2$ com classes mutuamente exclusivas. Exemplos: categorização de tópicos, classificação de emoções.

**Classificação Multi-label**: Cada instância pode pertencer a múltiplas classes simultaneamente. Exemplos: tagging de documentos, classificação de gêneros literários.

**Classificação Hierárquica**: Classes organizadas em hierarquia taxonômica. Exemplos: classificação de produtos, categorização científica.

### 2.3 Desafios Contemporâneos

**Dados Desbalanceados**: Distribuição não-uniforme de classes. Requer técnicas de balanceamento (oversampling, undersampling, SMOTE) ou ajuste de pesos de classes.

**Few-Shot Learning**: Generalização a partir de poucos exemplos por classe. Abordagens meta-learning e prompting de LLMs emergem como soluções.

**Domain Adaptation**: Transfer de conhecimento entre domínios com distribuições distintas. Técnicas incluem domain adversarial training e self-training.

**Adversarial Robustness**: Vulnerabilidade a exemplos adversariais (perturbações imperceptíveis que causam misclassification). Requer técnicas de adversarial training.

**Interpretabilidade**: Compreensão de rationale por trás de predições. Métodos incluem attention visualization, LIME, SHAP.

---

## 3. Métricas de Avaliação

### 3.1 Matriz de Confusão

Matriz de confusão é tabela de contingência que confronta predições com rótulos verdadeiros, base para derivação de diversas métricas.

**Classificação Binária**:

|                | Predito Positivo | Predito Negativo |
|----------------|------------------|------------------|
| **Real Positivo** | TP (True Positive) | FN (False Negative) |
| **Real Negativo** | FP (False Positive) | TN (True Negative) |

**Classificação Multiclasse**:

Matriz $k \times k$ onde elemento $C_{ij}$ representa número de instâncias da classe $i$ preditas como classe $j$.

### 3.2 Métricas Fundamentais

**Acurácia (Accuracy)**:

Proporção de predições corretas sobre total de predições.

$$\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}$$

*Limitação*: Enganosa para datasets desbalanceados. Sistema que sempre prediz classe majoritária pode ter alta acurácia.

**Precisão (Precision)**:

Proporção de predições positivas corretas sobre total de predições positivas.

$$\text{Precision} = \frac{TP}{TP + FP}$$

*Interpretação*: "Dos exemplos que classifiquei como positivos, quantos realmente são?"

**Revocação (Recall / Sensibilidade)**:

Proporção de positivos verdadeiros corretamente identificados.

$$\text{Recall} = \frac{TP}{TP + FN}$$

*Interpretação*: "Dos exemplos que são realmente positivos, quantos consegui identificar?"

**F1-Score**:

Média harmônica entre precisão e revocação, balanceando ambas.

$$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$$

**F-Beta Score**:

Generalização do F1 permitindo ponderar importância relativa de precisão vs. revocação.

$$F_\beta = (1 + \beta^2) \cdot \frac{\text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}$$

$\beta < 1$: enfatiza precisão; $\beta > 1$: enfatiza revocação.

### 3.3 Métricas Avançadas

**ROC-AUC (Receiver Operating Characteristic - Area Under Curve)**:

Curva ROC plota True Positive Rate (Recall) vs. False Positive Rate em diferentes thresholds de classificação.

$$\text{FPR} = \frac{FP}{FP + TN}$$

AUC (área sob curva ROC) quantifica performance agregada:
- AUC = 1.0: classificador perfeito
- AUC = 0.5: classificador aleatório
- AUC < 0.5: classificador invertido (worse than random)

*Vantagens*: Invariante a thresholds, robusto a classes desbalanceadas quando usado apropriadamente.

**Precision-Recall AUC**:

Para datasets extremamente desbalanceados, curva Precision-Recall (e respectiva AUC) pode ser mais informativa que ROC.

**Matthews Correlation Coefficient (MCC)**:

Métrica balanceada mesmo para datasets desbalanceados.

$$\text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$

Varia entre -1 (total disagreement) e +1 (perfect prediction), com 0 indicando predição aleatória.

**Métricas Multiclasse**:

Para classificação multiclasse, métricas binárias são estendidas mediante estratégias de agregação:

- **Macro-averaging**: Calcula métrica para cada classe e tira média não-ponderada
- **Micro-averaging**: Agrega TP, FP, FN globalmente antes de calcular métrica
- **Weighted-averaging**: Média ponderada por suporte (número de instâncias) de cada classe

### 3.4 Considerações sobre Classes Desbalanceadas

Datasets com distribuição não-uniforme de classes requerem atenção especial:

**Técnicas de Balanceamento**:
- **Oversampling**: SMOTE (Synthetic Minority Over-sampling Technique), ADASYN
- **Undersampling**: Random undersampling, Tomek links
- **Hybrid**: SMOTE + Tomek

**Ajuste de Pesos de Classe**: Penalizar erro em classes minoritárias mais fortemente durante treinamento.

**Métricas Apropriadas**: Priorizar F1-score (especialmente macro/weighted), Precision-Recall AUC, MCC sobre acurácia simples.

---

## 4. Abordagem 1: Embeddings Pré-treinados + KNN

### 4.1 Arquitetura e Fundamentos

Esta abordagem representa método de complexidade mínima, combinando embeddings de modelos de linguagem pré-treinados com classificador K-Nearest Neighbors (KNN).

**Pipeline**:

1. **Geração de Embeddings**: Textos são transformados em vetores densos mediante modelo pré-treinado (e.g., Sentence-BERT)
2. **Armazenamento**: Embeddings do conjunto de treino são indexados
3. **Classificação**: Para nova instância, k vizinhos mais próximos são recuperados; classe majoritária determina predição

**Fundamentos do KNN**:

KNN é algoritmo não-paramétrico, instance-based. Classificação baseia-se em "voto majoritário" dos k vizinhos mais próximos no espaço de embeddings.

**Escolha de k**: Hiperparâmetro crítico.
- k pequeno: baixo bias, alto variance (overfitting)
- k grande: alto bias, baixo variance (underfitting)
- Típico: k = √n ou determinado via validação cruzada

**Métricas de Distância**:
- **Euclidiana**: $d(x,y) = \sqrt{\sum_i (x_i - y_i)^2}$
- **Cosseno**: $d(x,y) = 1 - \frac{x \cdot y}{\|x\| \|y\|}$ (preferida para embeddings normalizados)
- **Manhattan**: $d(x,y) = \sum_i |x_i - y_i|$

### 4.2 Implementação Prática

**Seleção de Modelo de Embedding**:

- **all-MiniLM-L6-v2**: Leve (22M parâmetros), rápido, 384 dimensões, excelente para prototipagem
- **all-mpnet-base-v2**: Performance superior (110M parâmetros), 768 dimensões
- **multi-qa-MiniLM-L6-cos-v1**: Otimizado para tarefas question-answering
- **paraphrase-multilingual-mpnet-base-v2**: Suporte multilíngue

**Otimizações**:

- **Normalização de Embeddings**: Facilita uso de similaridade cosseno
- **Dimensionality Reduction**: PCA ou UMAP para reduzir dimensionalidade se necessário
- **Approximate Nearest Neighbors**: Para datasets grandes, usar HNSW ou FAISS em vez de busca exaustiva

### 4.3 Vantagens e Limitações

**Vantagens**:

1. **Simplicidade Extrema**: Implementação requer poucas linhas de código
2. **Sem Treinamento**: Aproveita conhecimento pré-treinado diretamente
3. **Interpretabilidade**: Vizinhos recuperados fornecem explicação intuitiva
4. **Atualização Incremental**: Novos exemplos adicionados sem re-treinamento
5. **Eficiência Computacional**: Sem GPUs necessárias, rápido para datasets moderados

**Limitações**:

1. **Performance Subótima**: Geralmente inferior a métodos que adaptam modelo ao domínio
2. **Escalabilidade**: Busca exaustiva impraticável para milhões de exemplos (requer ANN)
3. **Sensibilidade a k e Métrica**: Hiperparâmetros críticos, requerem tuning
4. **Curse of Dimensionality**: Performance degrada em espaços de alta dimensionalidade
5. **Sem Adaptação ao Domínio**: Embeddings genéricos podem não capturar nuances específicas

**Quando Utilizar**:

- Prototipagem rápida e baseline establishment
- Datasets pequenos a moderados (< 100k exemplos)
- Aplicações onde interpretabilidade é crucial
- Cenários com dados de treino limitados mas embeddings de qualidade disponíveis
- Budget computacional limitado (sem acesso a GPUs)

---

## 5. Abordagem 2: Fine-tuning de Modelos Pré-treinados

### 5.1 Transfer Learning e Fine-tuning

Transfer learning explora conhecimento adquirido em tarefa fonte (pré-treinamento em vasto corpus) para melhorar aprendizado em tarefa alvo (classificação específica).

**Paradigma de Pré-treinamento + Fine-tuning**:

1. **Pré-treinamento**: Modelo treinado em objetivo self-supervised massivo (e.g., masked language modeling)
2. **Fine-tuning**: Camada de classificação adicionada; modelo inteiro (ou subset de camadas) é retreinado em dataset supervisionado específico

**Vantagens de Transfer Learning**:

- Requer significativamente menos dados labeled que treinamento from-scratch
- Convergência mais rápida
- Melhor generalização devido a conhecimento linguístico pré-adquirido
- Especialmente eficaz para domínios com dados escassos

### 5.2 Arquitetura: DistilBERT

DistilBERT é versão destilada do BERT, retendo 97% da performance com 40% menos parâmetros e 60% maior velocidade.

**Processo de Destilação**:

Knowledge distillation transfere conhecimento de modelo grande (teacher) para modelo menor (student) mediante:

1. **Soft Targets**: Student minimiza divergência entre suas predições e distribuição de probabilidades do teacher (soft labels)
2. **Hidden State Matching**: Opcional, alinhar representações internas
3. **Hard Labels**: Adicionalmente, minimizar loss em labels verdadeiros

**Arquitetura**:

- 6 camadas transformer (vs. 12 do BERT)
- 768 dimensões hidden state
- 66M parâmetros (vs. 110M do BERT)
- Mantém mecanismo de atenção multi-head
- Token embeddings, positional embeddings, segment embeddings

### 5.3 Processo de Treinamento

**Preparação de Dados**:

1. **Tokenização**: Conversão de texto em tokens mediante tokenizer do modelo (WordPiece para BERT/DistilBERT)
2. **Padding**: Sequências ajustadas para comprimento uniforme
3. **Attention Masks**: Indicam tokens reais vs. padding
4. **Label Encoding**: Classes convertidas para índices inteiros

**Adição de Camada de Classificação**:

Sobre representação [CLS] (output da primeira posição), adiciona-se camada linear:

$$y = W_{cls} \cdot h_{[CLS]} + b_{cls}$$

onde $h_{[CLS]} \in \mathbb{R}^{768}$ e $W_{cls} \in \mathbb{R}^{k \times 768}$ para $k$ classes.

**Função de Perda**:

Cross-entropy multi-classe:

$$\mathcal{L} = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^k y_{i,c} \log(\hat{y}_{i,c})$$

onde $y_{i,c}$ é one-hot encoding e $\hat{y}_{i,c}$ é softmax output.

**Otimização**:

- **Otimizador**: AdamW (Adam com weight decay corrigido)
- **Learning Rate**: Tipicamente 2e-5 a 5e-5 (menor que from-scratch devido a pré-treinamento)
- **Learning Rate Scheduling**: Linear decay com warmup (500-2000 steps)
- **Batch Size**: 16-32 (limitado por memória GPU)
- **Epochs**: 2-4 (mais epochs risco de overfitting)
- **Gradient Clipping**: Clipar gradientes em norma 1.0

### 5.4 Estratégias de Otimização

**Regularização**:

- **Dropout**: Aplicado em camada de classificação (tipicamente 0.1)
- **Weight Decay**: Regularização L2 (tipicamente 0.01)
- **Early Stopping**: Monitorar loss de validação, parar se não melhorar

**Hyperparameter Tuning**:

- Grid search ou Bayesian optimization para learning rate, batch size, epochs
- Validação cruzada k-fold para datasets pequenos

**Técnicas Avançadas**:

- **Gradual Unfreezing**: Inicialmente treinar apenas classificador, gradualmente descongelar camadas inferiores
- **Discriminative Fine-tuning**: Learning rates distintos por camada (inferior = menor LR)
- **Layer-wise Learning Rate Decay**: LR decai exponencialmente de camadas superiores para inferiores
- **Adversarial Training**: Adicionar perturbações adversariais durante treinamento para robustez

**Considerações de Recursos**:

- **Memória GPU**: DistilBERT requer ~6GB para batch size 16, max length 128
- **Tempo de Treinamento**: ~30min-2h para datasets típicos (10k-100k exemplos) em GPU moderna
- **Mixed Precision Training**: FP16 reduz uso de memória e acelera treinamento

---

## 6. Abordagem 3: Classificação com LLMs

### 6.1 Prompt Engineering para Classificação

Large Language Models (LLMs) como GPT-3/4, Gemini, Claude possibilitam classificação mediante prompting, sem necessidade de fine-tuning.

**Estrutura de Prompt para Classificação**:

```
Você é um sistema especializado em classificação de texto.

Categorias disponíveis: [lista de classes]

[Opcional: Descrição de cada categoria]

[Opcional: Few-shot examples]

Classifique o seguinte texto em uma das categorias acima.

Texto: "[texto a classificar]"

Responda apenas com o nome da categoria.
```

**Componentes Críticos**:

1. **System Prompt**: Define papel e comportamento do modelo
2. **Task Description**: Especifica claramente a tarefa
3. **Categories**: Lista explícita de classes permitidas
4. **Examples**: Few-shot examples demonstram formato esperado
5. **Input**: Texto a classificar
6. **Output Format**: Instrução explícita sobre formato de resposta

### 6.2 Zero-shot vs. Few-shot Learning

**Zero-shot Classification**:

Modelo classifica sem ver exemplos da tarefa, baseando-se apenas em descrição das categorias.

*Vantagens*: Sem necessidade de dados labeled
*Limitações*: Performance inferior, especialmente para tarefas com nuances sutis

**Few-shot Classification**:

Incluir poucos exemplos (tipicamente 1-10) de cada classe no prompt.

*Exemplo Few-shot*:

```
Exemplo 1:
Texto: "Este produto é maravilhoso! Superou expectativas."
Categoria: Positivo

Exemplo 2:
Texto: "Qualidade péssima, não recomendo."
Categoria: Negativo

Agora classifique:
Texto: "Funciona bem mas poderia ser mais barato."
```

*Vantagens*: Performance significativamente melhor que zero-shot
*Limitações*: Context window limitado restringe número de examples

**Estratégias para Seleção de Examples**:

- **Representativos**: Cobrir diversidade de cada classe
- **Desafiadores**: Incluir casos limítrofes
- **Balanceados**: Número similar de examples por classe
- **Recentes**: Para domínios temporais, exemplos recentes

### 6.3 Structured Output e Validação

**Structured Output**:

Para garantir outputs estruturados e parseáveis, usar:

1. **Function Calling / Tools**: APIs que forçam JSON schema (OpenAI, Anthropic)
2. **Response Schema**: Especificar schema explícito (Gemini response_schema)
3. **Grammar-based Sampling**: Constrain geração para seguir gramática (LM Studio, Llama.cpp)

**Exemplo com Gemini**:

```python
from enum import Enum

class Sentiment(Enum):
    POSITIVE = "positivo"
    NEGATIVE = "negativo"
    NEUTRAL = "neutro"

response = client.generate_content(
    prompt=prompt,
    config={
        "response_mime_type": "text/x.enum",
        "response_schema": Sentiment
    }
)
```

**Validação de Outputs**:

Mesmo com structured output, validar:
- Output pertence ao conjunto de classes válidas
- Formato correto (caso JSON)
- Fallback para classe default ou re-try em caso de invalidez

### 6.4 Considerações de Custo e Latência

**Custo**:

LLMs comerciais cobram por tokens (input + output).

*Exemplos de pricing (GPT-4o, aproximados)*:
- Input: $2.50 / 1M tokens
- Output: $10.00 / 1M tokens

Para 10k classificações com prompts de 100 tokens e output de 5 tokens:
- Input: 10k × 100 = 1M tokens → $2.50
- Output: 10k × 5 = 50k tokens → $0.50
- **Total: ~$3.00**

Comparar com custo de fine-tuning (one-time) + inference de modelo próprio.

**Latência**:

Chamadas API introduzem latência de rede + tempo de geração.

*Típico*:
- Zero-shot: 500ms - 2s por classificação
- Few-shot: 1s - 5s (devido a prompt maior)

Para aplicações high-throughput, latência pode ser proibitiva.

**Rate Limits**:

APIs impõem limites (e.g., 10 req/min para tier free, 500+ req/min para tier pago). Requer:
- Batching de requests
- Exponential backoff retry logic
- Caching de resultados quando possível

**Quando Utilizar LLMs**:

- **Dados limitados**: Poucos/nenhum exemplo labeled disponível
- **Prototipagem rápida**: Validar viabilidade antes de investir em fine-tuning
- **Tarefas complexas**: Classificação requer reasoning sofisticado
- **Multi-task**: Mesmo modelo serve múltiplas tarefas via prompts distintos
- **Domain shift frequente**: Evitar re-training constante

**Quando Evitar**:

- **Alto volume**: Custo e latência tornam-se proibitivos
- **Tempo real**: Requisitos de latência ultra-baixa (<100ms)
- **Privacidade**: Dados sensíveis não podem ser enviados a APIs externas
- **Budget limitado**: Fine-tuning de modelo menor pode ser mais econômico long-term

---

## 7. Análise Comparativa das Abordagens

### 7.1 Comparação de Performance

Performance varia significativamente conforme dataset e domínio. Resultados típicos em benchmarks padrão:

**Emotion Dataset** (6 classes: sadness, joy, love, anger, fear, surprise):

| Abordagem | Accuracy | F1 (Weighted) | Tempo Treino | Tempo Inferência (100 amostras) |
|-----------|----------|---------------|--------------|----------------------------------|
| Embedding + KNN | 0.85 | 0.84 | 0s | 2s |
| Fine-tuned DistilBERT | 0.93 | 0.92 | 30min | 5s |
| LLM (Few-shot) | 0.89 | 0.88 | 0s | 120s |

**IMDB Sentiment** (2 classes: positive, negative):

| Abordagem | Accuracy | F1 | ROC-AUC | Tempo Treino | Tempo Inferência |
|-----------|----------|-----|---------|--------------|-------------------|
| Embedding + KNN | 0.88 | 0.88 | 0.94 | 0s | 3s |
| Fine-tuned DistilBERT | 0.95 | 0.95 | 0.98 | 45min | 6s |
| LLM (Few-shot) | 0.91 | 0.91 | 0.96 | 0s | 180s |

### 7.2 Análise de Trade-offs

**Dimensões de Comparação**:

1. **Performance (Acurácia)**:
   - Fine-tuning > LLM > Embeddings+KNN
   - Gap diminui para tarefas mais simples

2. **Custo Computacional (Treinamento)**:
   - Embeddings+KNN (sem treino) < LLM (sem treino) < Fine-tuning (requer GPU)

3. **Custo Computacional (Inferência)**:
   - Embeddings+KNN ≈ Fine-tuning < LLM (chamadas API)

4. **Custo Monetário**:
   - Baixo volume: LLM pode ser mais barato (sem infra)
   - Alto volume: Fine-tuning amortiza custo

5. **Latência**:
   - Embeddings+KNN (rápido) ≈ Fine-tuning (rápido) < LLM (lento)

6. **Requisitos de Dados**:
   - LLM (poucos/nenhum) < Embeddings+KNN (moderado) < Fine-tuning (substancial)

7. **Facilidade de Implementação**:
   - Embeddings+KNN (simples) ≈ LLM (simples) < Fine-tuning (complexo)

8. **Interpretabilidade**:
   - Embeddings+KNN (vizinhos) > LLM (chain-of-thought) > Fine-tuning (opaco)

9. **Adaptabilidade**:
   - LLM (prompt update) > Embeddings+KNN (add instances) > Fine-tuning (re-train)

### 7.3 Critérios de Seleção

**Escolha Embeddings + KNN quando**:
- Prototipagem rápida
- Interpretabilidade é crucial
- Dados de treino limitados mas embeddings de qualidade disponíveis
- Budget computacional inexistente
- Atualização incremental frequente

**Escolha Fine-tuning quando**:
- Máxima performance é requisito
- Dataset labeled substancial disponível (>1k exemplos por classe)
- Recursos computacionais disponíveis (GPU)
- Aplicação production com alto volume
- Latência baixa é crítica

**Escolha LLM quando**:
- Dados labeled escassos/inexistentes
- Prototipagem ultra-rápida
- Múltiplas tarefas relacionadas (multi-task)
- Classificação requer reasoning complexo
- Domain shift frequente (evitar re-training)
- Volume de predições moderado

**Abordagem Híbrida**:

Combinar múltiplas abordagens:
- **LLM para labeling**: Usar LLM para gerar labels, fine-tune modelo menor
- **Cascade**: KNN para casos fáceis (alta confiança), LLM para difíceis
- **Ensemble**: Combinar predições de múltiplas abordagens

---

## 8. Casos de Uso e Aplicações

### 8.1 Análise de Sentimento

**Descrição**: Classificar texto segundo polaridade emocional (positivo, negativo, neutro).

**Domínios**:
- Reviews de produtos/serviços
- Análise de mídias sociais (brand monitoring)
- Feedback de clientes
- Análise de mercado financeiro (sentiment sobre stocks)

**Desafios Específicos**:
- Sarcasmo e ironia
- Sentimento misto (aspectos positivos e negativos coexistem)
- Domain-specific expressions

**Abordagem Recomendada**: Fine-tuning para aplicações production; LLM para prototipagem ou domínios novos.

### 8.2 Classificação de Documentos

**Descrição**: Categorizar documentos em tópicos/categorias pré-definidas.

**Aplicações**:
- Organização de knowledge bases
- Roteamento automático de emails/tickets
- Classificação de artigos científicos
- Categorização de produtos em e-commerce

**Desafios**:
- Hierarquias de categorias complexas
- Documentos longos (excedem context window)
- Classes desbalanceadas

**Estratégias**:
- Chunking + agregação para documentos longos
- Classificação hierárquica multi-stage
- Balanceamento via weighted loss ou sampling

### 8.3 Detecção de Spam e Conteúdo Tóxico

**Descrição**: Identificar conteúdo indesejado, malicioso ou nocivo.

**Tipos**:
- **Spam**: Email spam, spam em comentários/reviews
- **Toxicidade**: Hate speech, harassment, ameaças
- **Desinformação**: Fake news, clickbait

**Características**:
- Classes extremamente desbalanceadas (spam << legítimo)
- Evolução adversarial constante (spammers adaptam táticas)
- Criticidade de falsos positivos (legítimo marcado como spam)

**Abordagens**:
- Fine-tuning com heavy regularization
- Active learning para adaptar a novas táticas
- Ensemble de múltiplos modelos
- Threshold tuning para balancear precision/recall

### 8.4 Aplicações Especializadas

**Healthcare**:
- Classificação de notas médicas
- Triagem de sintomas
- Detecção de eventos adversos em relatórios

**Legal**:
- Classificação de contratos por tipo
- Identificação de cláusulas específicas
- Predição de outcomes de casos

**Finanças**:
- Classificação de transações
- Detecção de fraude
- Categorização de despesas

**Customer Support**:
- Roteamento inteligente de tickets
- Classificação de urgência
- Identificação de intent

---

## 9. Conclusões e Perspectivas Futuras

Este documento apresentou análise sistemática das abordagens contemporâneas para classificação textual com IA, abrangendo desde métodos leves baseados em embeddings até fine-tuning de transformers e prompting de LLMs de última geração.

**Principais Conclusões**:

1. **Não Há Abordagem Universalmente Ótima**: Seleção depende criticamente de constraints (dados, compute, latência, custo) e requisitos (performance, interpretabilidade).

2. **Trade-off Performance vs. Complexidade**: Fine-tuning oferece máxima performance mas requer dados, expertise e recursos. Embeddings+KNN e LLMs democratizam acesso com trade-offs distintos.

3. **Transfer Learning Revolucionou o Campo**: Modelos pré-treinados eliminaram necessidade de datasets massivos e treinamento from-scratch na maioria dos casos.

4. **LLMs Abrem Novas Possibilidades**: Paradigma zero/few-shot via prompting habilita classificação sem dados labeled, especialmente valioso para domínios com escassez de labels.

5. **Métricas Importam**: Para datasets desbalanceados (maioria dos casos reais), acurácia é enganosa. F1-score, PRAUC, MCC devem ser priorizados.

**Tendências Emergentes**:

**1. Modelos Maiores e Mais Capazes**:
- GPT-4, Gemini Ultra, Claude 3 Opus demonstram capacidades emergentes impressionantes
- Especialização via fine-tuning de modelos massivos (>100B parâmetros)

**2. Efficient Fine-tuning**:
- **LoRA** (Low-Rank Adaptation): Fine-tune apenas matrizes low-rank, reduz parâmetros treináveis em 10000×
- **QLoRA**: Quantização + LoRA, permite fine-tuning de LLMs em GPUs consumer
- **Prefix Tuning**, **Adapter Layers**: Alternativas parameter-efficient

**3. Multimodalidade**:
- Modelos que processam texto + imagem + áudio (GPT-4V, Gemini)
- Classificação baseada em múltiplas modalidades simultaneamente

**4. Retrieval-Augmented Classification**:
- Combinar classificação com retrieval de exemplos relevantes
- Usar retrieved examples como few-shot prompts dinâmicos

**5. Continual Learning**:
- Modelos que adaptam continuamente a novos dados sem catastrophic forgetting
- Crítico para domínios em evolução

**6. Explainable AI**:
- Demand crescente por interpretabilidade em aplicações críticas (healthcare, legal, finance)
- Técnicas de attribution, conterfactual explanations, attention visualization

**7. Privacy-Preserving ML**:
- Federated Learning: Treinar sem centralizar dados sensíveis
- Differential Privacy: Garantias formais de privacidade
- Homomorphic Encryption: Computação sobre dados encriptados

**Direções de Pesquisa**:

- **Robustness**: Defesa contra adversarial attacks, out-of-distribution detection
- **Fairness**: Mitigação de biases sociais, paridade demográfica
- **Few-shot Learning**: Meta-learning, prototype networks
- **Active Learning**: Seleção inteligente de exemplos para labeling humano
- **Multi-task Learning**: Single model que serve múltiplas tarefas relacionadas

**Palavras Finais**:

A classificação textual com IA é campo vibrante em rápida evolução. Profissionais devem manter-se atualizados com desenvolvimentos, experimentar empiricamente múltiplas abordagens, e selecionar técnicas apropriadas aos constraints e requisitos específicos de cada aplicação. O futuro promete modelos cada vez mais capazes, eficientes e acessíveis.

---

## Apêndices

### Apêndice A: Formulações Matemáticas

**Softmax** (para outputs de classificação multiclasse):

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}$$

**Cross-Entropy Loss** (classificação multiclasse):

$$\mathcal{L}_{CE} = -\sum_{c=1}^k y_c \log(\hat{y}_c)$$

onde $y_c \in \{0,1\}$ (one-hot) e $\hat{y}_c$ é probabilidade predita.

**Binary Cross-Entropy** (classificação binária):

$$\mathcal{L}_{BCE} = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$$

**Focal Loss** (para classes desbalanceadas):

$$\mathcal{L}_{FL} = -\alpha_t (1-p_t)^\gamma \log(p_t)$$

onde $p_t$ é probabilidade da classe verdadeira, $\alpha$ balanceia classes, $\gamma$ reduz peso de exemplos fáceis.

**KNN Decision Rule**:

$$\hat{y} = \arg\max_{c \in C} \sum_{x_i \in N_k(x)} \mathbb{1}(y_i = c)$$

onde $N_k(x)$ são k vizinhos mais próximos de $x$.

**F-Beta Score**:

$$F_\beta = (1+\beta^2) \frac{P \cdot R}{\beta^2 P + R}$$

**MCC (Matthews Correlation Coefficient)**:

$$\text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$

**Cohen's Kappa** (concordância inter-rater):

$$\kappa = \frac{p_o - p_e}{1 - p_e}$$

onde $p_o$ é observed agreement, $p_e$ é expected agreement por chance.

---

### Apêndice B: Exemplos de Código

**Exemplo 1: Embeddings + KNN**

```python
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

# Carregar modelo de embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Dados de exemplo
train_texts = [
    "Este produto é excelente!",
    "Qualidade muito boa, recomendo.",
    "Péssimo, não funciona.",
    "Muito ruim, decepcionante."
]
train_labels = ["positivo", "positivo", "negativo", "negativo"]

# Gerar embeddings
train_embeddings = model.encode(train_texts, normalize_embeddings=True)

# Treinar KNN
knn = KNeighborsClassifier(n_neighbors=3, metric='cosine')
knn.fit(train_embeddings, train_labels)

# Classificar novo texto
test_text = "Produto bom mas poderia ser melhor"
test_embedding = model.encode([test_text], normalize_embeddings=True)
prediction = knn.predict(test_embedding)
probabilities = knn.predict_proba(test_embedding)

print(f"Texto: {test_text}")
print(f"Predição: {prediction[0]}")
print(f"Probabilidades: {dict(zip(knn.classes_, probabilities[0]))}")
```

**Exemplo 2: Fine-tuning DistilBERT**

```python
from transformers import (
    DistilBertTokenizer,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
import torch

# Preparar dados
texts = ["Texto 1", "Texto 2", "Texto 3"]
labels = [0, 1, 0]

dataset = Dataset.from_dict({"text": texts, "label": labels})

# Tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Modelo
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=2
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_steps=10,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# Treinar
trainer.train()

# Predição
inputs = tokenizer("Novo texto", return_tensors="pt", truncation=True, max_length=128)
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits
    probs = torch.nn.functional.softmax(logits, dim=1)
    prediction = torch.argmax(probs, dim=1)

print(f"Predição: {prediction.item()}")
print(f"Probabilidades: {probs.numpy()}")
```

**Exemplo 3: Classificação com LLM (Gemini)**

```python
from google import genai
from enum import Enum

# Definir enum de categorias
class Sentiment(Enum):
    POSITIVO = "positivo"
    NEGATIVO = "negativo"
    NEUTRO = "neutro"

# Cliente e classificação
client = genai.Client(api_key="YOUR_API_KEY")

text = "Funciona bem mas é caro"
prompt = "Classifique o sentimento do seguinte texto como positivo, negativo ou neutro: " + text

response = client.models.generate_content(
    model="gemini-2.0-flash-exp",
    contents=prompt,
    config={
        "response_mime_type": "text/x.enum",
        "response_schema": Sentiment
    }
)

print(f"Texto: {text}")
print(f"Classificação: {response.text}")
```

**Exemplo 4: Cálculo de Métricas**

```python
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
    roc_auc_score
)
import numpy as np

# Dados de exemplo
y_true = [0, 1, 0, 1, 1, 0, 1, 0, 1, 1]
y_pred = [0, 1, 0, 1, 0, 0, 1, 0, 1, 1]
y_prob = [
    [0.9, 0.1], [0.2, 0.8], [0.8, 0.2], [0.1, 0.9], [0.6, 0.4],
    [0.95, 0.05], [0.3, 0.7], [0.85, 0.15], [0.2, 0.8], [0.1, 0.9]
]

# Acurácia
accuracy = accuracy_score(y_true, y_pred)
print(f"Accuracy: {accuracy:.3f}")

# Precision, Recall, F1
precision, recall, f1, support = precision_recall_fscore_support(
    y_true, y_pred, average='binary'
)
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")

# Matriz de confusão
cm = confusion_matrix(y_true, y_pred)
print(f"\nConfusion Matrix:\n{cm}")

# ROC-AUC
y_prob_positive = np.array(y_prob)[:, 1]
roc_auc = roc_auc_score(y_true, y_prob_positive)
print(f"\nROC-AUC: {roc_auc:.3f}")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=['Negativo', 'Positivo']))
```

---

### Apêndice C: Comparação Detalhada das Abordagens

| Critério | Embeddings + KNN | Fine-tuning | LLM Prompting |
|----------|------------------|-------------|---------------|
| **Performance** | ⭐⭐⭐ (70-85%) | ⭐⭐⭐⭐⭐ (90-97%) | ⭐⭐⭐⭐ (85-93%) |
| **Dados Necessários** | Moderado (100+ ex/classe) | Alto (1k+ ex/classe) | Baixo (0-10 ex/classe) |
| **Tempo de Setup** | Minutos | Horas | Minutos |
| **Custo de Treino** | $ (sem GPU) | $$$ (GPU necessária) | $ (sem treino) |
| **Custo de Inferência** | $ (CPU barato) | $$ (GPU recomendada) | $$$ (API calls) |
| **Latência** | Rápida (10-50ms) | Rápida (20-100ms) | Lenta (500-5000ms) |
| **Escalabilidade Treino** | N/A | Moderada | N/A |
| **Escalabilidade Inferência** | Boa (com ANN) | Boa | Limitada (rate limits) |
| **Interpretabilidade** | ⭐⭐⭐⭐⭐ (vizinhos) | ⭐⭐ (atenção) | ⭐⭐⭐ (chain-of-thought) |
| **Adaptabilidade** | ⭐⭐⭐⭐ (add data) | ⭐⭐ (re-train) | ⭐⭐⭐⭐⭐ (edit prompt) |
| **Robustness** | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Facilidade Implementação** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |
| **Expertise Requerida** | Baixa | Alta | Média |
| **Manutenção** | Baixa | Média-Alta | Baixa |

**Legenda**:
- $ = baixo custo, $$$ = alto custo
- ⭐ = ruim, ⭐⭐⭐⭐⭐ = excelente

---

### Apêndice D: Datasets de Benchmark

**1. IMDB Movie Reviews**:
- **Descrição**: 50k reviews de filmes (25k treino, 25k teste)
- **Classes**: 2 (positivo, negativo)
- **Desafio**: Textos longos, linguagem informal, sarcasmo
- **URL**: https://ai.stanford.edu/~amaas/data/sentiment/

**2. Emotion Dataset**:
- **Descrição**: 20k tweets anotados com emoções
- **Classes**: 6 (sadness, joy, love, anger, fear, surprise)
- **Desafio**: Textos curtos, classes desbalanceadas
- **URL**: https://huggingface.co/datasets/emotion

**3. AG News**:
- **Descrição**: 120k artigos de notícias
- **Classes**: 4 (World, Sports, Business, Sci/Tech)
- **Desafio**: Tópicos sobrepostos, textos médios
- **URL**: https://huggingface.co/datasets/ag_news

**4. DBpedia**:
- **Descrição**: 630k documentos de Wikipedia
- **Classes**: 14 (Company, Artist, Athlete, etc.)
- **Desafio**: Hierarquia de classes, textos longos
- **URL**: https://huggingface.co/datasets/dbpedia_14

**5. 20 Newsgroups**:
- **Descrição**: ~20k documentos de newsgroups
- **Classes**: 20 (comp.graphics, sci.med, etc.)
- **Desafio**: Classes finamente granulares, sobreposição temática
- **URL**: scikit-learn.datasets.fetch_20newsgroups

**6. Toxic Comment Classification**:
- **Descrição**: 160k comentários do Wikipedia
- **Classes**: 6 (toxic, severe_toxic, obscene, threat, insult, identity_hate) - multi-label
- **Desafio**: Multi-label, classes raras, linguistic diversity
- **URL**: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge

**7. TREC Question Classification**:
- **Descrição**: 6k perguntas classificadas por tipo
- **Classes**: 6 (ABBR, ENTITY, DESCRIPTION, HUMAN, LOCATION, NUMERIC)
- **Desafio**: Textos muito curtos, nuances semânticas
- **URL**: https://cogcomp.seas.upenn.edu/Data/QA/QC/

---

### Apêndice E: Glossário

**Attention Mechanism**: Mecanismo que permite modelo focar em partes relevantes da entrada durante processamento.

**BERT** (Bidirectional Encoder Representations from Transformers): Modelo transformer bidirecional pré-treinado via masked language modeling.

**Class Imbalance**: Distribuição não-uniforme de classes em dataset, podendo enviesar modelo para classes majoritárias.

**Confusion Matrix**: Tabela que confronta predições com rótulos verdadeiros, base para cálculo de métricas.

**Cross-Entropy**: Função de perda que mede divergência entre distribuição verdadeira e predita.

**Embedding**: Representação vetorial densa de dados discretos (palavras, sentenças) em espaço contínuo.

**Few-Shot Learning**: Aprendizado a partir de poucos exemplos (tipicamente 1-10) por classe.

**Fine-tuning**: Adaptação de modelo pré-treinado a tarefa específica mediante treinamento adicional.

**F1-Score**: Média harmônica entre precisão e revocação, balanceando ambas métricas.

**Knowledge Distillation**: Transferência de conhecimento de modelo grande (teacher) para modelo pequeno (student).

**KNN** (K-Nearest Neighbors): Algoritmo de classificação baseado em voto majoritário dos k vizinhos mais próximos.

**LLM** (Large Language Model): Modelo de linguagem de grande escala (bilhões de parâmetros) com capacidades emergentes.

**LoRA** (Low-Rank Adaptation): Técnica de fine-tuning parameter-efficient mediante decomposição low-rank.

**Masked Language Modeling**: Objetivo de pré-treinamento onde tokens mascarados devem ser preditos a partir de contexto.

**Precision**: Proporção de predições positivas que são corretas.

**Prompt Engineering**: Arte de craftar prompts eficazes para elicitar comportamento desejado de LLMs.

**Recall**: Proporção de positivos verdadeiros corretamente identificados.

**ROC-AUC**: Área sob curva ROC (True Positive Rate vs. False Positive Rate), métrica agregada de performance.

**Softmax**: Função que converte logits em distribuição de probabilidades.

**Tokenization**: Processo de segmentar texto em tokens (palavras, subpalavras, caracteres).

**Transfer Learning**: Aplicação de conhecimento adquirido em tarefa fonte para melhorar performance em tarefa alvo.

**Transformer**: Arquitetura neural baseada em atenção, fundamento de modelos modernos de linguagem.

**Zero-Shot Learning**: Classificação sem exemplos da tarefa, baseando-se apenas em descrição de classes.

---

### Apêndice F: Perguntas Frequentes

**1. Qual abordagem escolher para começar?**

Para prototipagem inicial, recomenda-se **Embeddings + KNN** pela simplicidade e rapidez. Se performance é insatisfatória e há dados labeled suficientes (>1k exemplos/classe), **fine-tuning** é próximo passo. Para cenários com poucos dados, **LLM few-shot** é alternativa viável.

**2. Quantos dados são necessários para fine-tuning?**

Depende de complexidade da tarefa e similaridade com domínio de pré-treinamento. Regra geral:
- **Mínimo**: 100-200 exemplos/classe
- **Recomendado**: 1k-10k exemplos/classe
- **Ideal**: 10k+ exemplos/classe

Datasets menores requerem regularização agressiva e data augmentation.

**3. Como lidar com classes desbalanceadas?**

Estratégias:
- **Dados**: Oversampling (SMOTE), undersampling, data augmentation
- **Treinamento**: Class weights, focal loss
- **Avaliação**: Usar F1, ROC-AUC, PR-AUC em vez de accuracy
- **Threshold tuning**: Ajustar threshold de decisão para balancear precision/recall

**4. LLMs são sempre melhores que fine-tuning?**

Não. Fine-tuning geralmente supera LLMs em:
- Performance absoluta (quando dados suficientes disponíveis)
- Latência (modelos menores mais rápidos)
- Custo (para alto volume)
- Controle (modelo próprio vs. API externa)

LLMs excellem em:
- Poucos/nenhum dado labeled
- Prototipagem rápida
- Tarefas requerendo reasoning complexo

**5. Como avaliar modelo em produção?**

Métricas offline (F1, etc.) são insuficientes. Monitorar:
- **Performance metrics**: Calcular métricas em amostra de predições
- **Latência**: p50, p95, p99
- **Throughput**: Requisições/segundo
- **Drift**: Distribuição de inputs e outputs ao longo do tempo
- **Business metrics**: Impacto em KPIs de negócio

Implementar logging, A/B testing, e re-treinamento periódico.

**6. Como interpretar predições de modelos?**

Técnicas:
- **KNN**: Examinar vizinhos recuperados
- **Fine-tuning**: Attention visualization, LIME, SHAP
- **LLM**: Chain-of-thought prompting (pedir justificativa)

**7. Qual modelo de embedding usar?**

Depende de requisitos:
- **Velocidade**: all-MiniLM-L6-v2 (22M parâmetros, 384 dim)
- **Qualidade**: all-mpnet-base-v2 (110M parâmetros, 768 dim)
- **Multilíngue**: paraphrase-multilingual-mpnet-base-v2
- **Domínio específico**: Buscar modelos especializados (e.g., BioBERT para biomédico)

**8. Fine-tuning completo vs. parameter-efficient (LoRA)?**

**Full fine-tuning**:
- Maior performance
- Requer mais memória GPU
- Treinamento mais lento

**LoRA**:
- Performance próxima (95-99% de full)
- Reduz parâmetros treináveis em 10000×
- Permite fine-tuning de modelos grandes em GPUs consumer

Para modelos >1B parâmetros, LoRA é preferível.

**9. Como escolher threshold de classificação?**

Threshold padrão (0.5 para binário) pode não ser ótimo. Estratégias:
- **ROC curve**: Escolher ponto que maximiza desired metric
- **Precision-Recall curve**: Para datasets desbalanceados
- **Business constraints**: E.g., garantir precision >95% (tolerando recall menor)

Use validation set para tuning.

**10. Modelos multilingues vs. modelos específicos de idioma?**

**Multilingues** (e.g., mBERT, XLM-RoBERTa):
- Convenientes para múltiplos idiomas
- Performance ~5-10% inferior a modelos monolíngues
- Úteis para transfer cross-lingual

**Monolíngues** (e.g., BERTimbau para português):
- Máxima performance para idioma específico
- Recomendados quando foco em single language

Se aplicação é multilíngue, multilingue é pragmático; se single-language, monolíngue é preferível.

---

## Referências

**Artigos Fundamentais**:

1. Devlin, J., et al. (2018). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". NAACL.

2. Sanh, V., et al. (2019). "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter". NeurIPS Workshop.

3. Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks". EMNLP.

4. Brown, T., et al. (2020). "Language Models are Few-Shot Learners". NeurIPS.

5. Liu, Y., et al. (2019). "RoBERTa: A Robustly Optimized BERT Pretraining Approach". arXiv:1907.11692.

**Transfer Learning e Fine-tuning**:

6. Howard, J., & Ruder, S. (2018). "Universal Language Model Fine-tuning for Text Classification". ACL.

7. Hu, E., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models". ICLR.

8. Dettmers, T., et al. (2023). "QLoRA: Efficient Finetuning of Quantized LLMs". NeurIPS.

**Métricas e Avaliação**:

9. Sokolova, M., & Lapalme, G. (2009). "A systematic analysis of performance measures for classification tasks". Information Processing & Management.

10. Japkowicz, N., & Shah, M. (2011). "Evaluating Learning Algorithms: A Classification Perspective". Cambridge University Press.

**Prompt Engineering**:

11. Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". NeurIPS.

12. Reynolds, L., & McDonell, K. (2021). "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm". CHI Extended Abstracts.

**Datasets e Benchmarks**:

13. Maas, A., et al. (2011). "Learning Word Vectors for Sentiment Analysis". ACL.

14. Zhang, X., et al. (2015). "Character-level Convolutional Networks for Text Classification". NIPS.

15. Wang, A., et al. (2019). "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding". ICLR.

**Livros**:

16. Jurafsky, D., & Martin, J. H. (2023). "Speech and Language Processing" (3rd ed.). Draft.

17. Goodfellow, I., et al. (2016). "Deep Learning". MIT Press.

18. Tunstall, L., et al. (2022). "Natural Language Processing with Transformers". O'Reilly.

**Recursos e Ferramentas**:

19. HuggingFace Transformers. <https://huggingface.co/transformers/>

20. Sentence Transformers. <https://www.sbert.net/>

21. Scikit-learn Documentation. <https://scikit-learn.org/>

---

**Última atualização**: 2025-11-04
**Versão**: 1.0
