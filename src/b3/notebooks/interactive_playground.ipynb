{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground Interativo - Testando os Classificadores\n",
    "\n",
    "**Execute este notebook AP√ìS treinar os modelos com `main.py`**\n",
    "\n",
    "Aqui voc√™ vai:\n",
    "- Testar os 3 modelos com seus pr√≥prios textos\n",
    "- Ver predi√ß√µes e probabilidades lado a lado\n",
    "- Comparar qual modelo √© melhor para diferentes tipos de texto\n",
    "- Experimentar e aprender interativamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from models.embedding_classifier import EmbeddingClassifier\n",
    "from models.finetuned_classifier import FinetunedClassifier\n",
    "from models.llm_classifier import LLMClassifier\n",
    "from utils.data_loader import load_and_prepare_dataset, get_few_shot_examples\n",
    "import config\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úì Imports carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar Modelos Treinados\n",
    "\n",
    "Isso pode levar alguns segundos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Carregando modelos...\\n\")\n",
    "\n",
    "# Carregar dataset para obter labels\n",
    "train_df, test_df, id2label, label2id = load_and_prepare_dataset(max_samples=1000)\n",
    "labels = list(id2label.values())\n",
    "\n",
    "# 1. Embedding Classifier\n",
    "print(\"[1/3] Carregando Embedding Classifier...\")\n",
    "embedding_clf = EmbeddingClassifier()\n",
    "try:\n",
    "    embedding_clf.load_model()\n",
    "    print(\"      ‚úì Modelo carregado de cache\")\n",
    "except:\n",
    "    print(\"      ‚ö†Ô∏è Modelo n√£o encontrado. Treinando do zero...\")\n",
    "    embedding_clf.fit(\n",
    "        texts=train_df['text'].tolist(),\n",
    "        labels=train_df['label'].tolist()\n",
    "    )\n",
    "    embedding_clf.save_model()\n",
    "    print(\"      ‚úì Modelo treinado e salvo\")\n",
    "\n",
    "# 2. Fine-tuned Classifier\n",
    "print(\"\\n[2/3] Carregando Fine-tuned Classifier...\")\n",
    "finetuned_clf = FinetunedClassifier(\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "try:\n",
    "    finetuned_clf.load_model()\n",
    "    print(\"      ‚úì Modelo carregado de cache\")\n",
    "except:\n",
    "    print(\"      ‚ö†Ô∏è Modelo n√£o encontrado. Execute 'python main.py' primeiro.\")\n",
    "    print(\"      Ou aguarde enquanto treino (pode levar 30-60 min)...\")\n",
    "    finetuned_clf.train(\n",
    "        train_texts=train_df['text'].tolist(),\n",
    "        train_labels=train_df['label'].tolist(),\n",
    "        epochs=1  # Apenas 1 √©poca para teste r√°pido\n",
    "    )\n",
    "    finetuned_clf.save_model()\n",
    "    print(\"      ‚úì Modelo treinado e salvo\")\n",
    "\n",
    "# 3. LLM Classifier\n",
    "print(\"\\n[3/3] Carregando LLM Classifier...\")\n",
    "few_shot_examples = get_few_shot_examples(train_df, id2label, num_examples=3)\n",
    "llm_clf = LLMClassifier(\n",
    "    labels=labels,\n",
    "    few_shot_examples=few_shot_examples\n",
    ")\n",
    "print(\"      ‚úì LLM pronto (Gemini API)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Todos os modelos carregados e prontos!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fun√ß√£o para Testar um Texto\n",
    "\n",
    "Esta fun√ß√£o testa um texto em todos os 3 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text(text: str, show_plot: bool = True):\n",
    "    \"\"\"\n",
    "    Testa um texto em todos os 3 classificadores.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto para classificar\n",
    "        show_plot: Se True, mostra gr√°fico de probabilidades\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Texto: {text}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Embedding\n",
    "    pred_embedding = embedding_clf.predict([text])[0]\n",
    "    proba_embedding = embedding_clf.predict_proba([text])[0]\n",
    "    results['Embedding + KNN'] = {\n",
    "        'prediction': id2label[pred_embedding],\n",
    "        'probabilities': proba_embedding\n",
    "    }\n",
    "    \n",
    "    # 2. Fine-tuned\n",
    "    pred_finetuned = finetuned_clf.predict([text])[0]\n",
    "    proba_finetuned = finetuned_clf.predict_proba([text])[0]\n",
    "    results['Fine-tuned DistilBERT'] = {\n",
    "        'prediction': id2label[pred_finetuned],\n",
    "        'probabilities': proba_finetuned\n",
    "    }\n",
    "    \n",
    "    # 3. LLM\n",
    "    pred_llm = llm_clf.predict([text])[0]\n",
    "    proba_llm = llm_clf.predict_proba([text])[0]\n",
    "    results['LLM (Gemini)'] = {\n",
    "        'prediction': labels[pred_llm],\n",
    "        'probabilities': proba_llm\n",
    "    }\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\nüìä PREDI√á√ïES:\\n\")\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        pred = result['prediction']\n",
    "        proba = result['probabilities']\n",
    "        confidence = proba.max() * 100\n",
    "        \n",
    "        print(f\"{model_name:<25}: {pred.upper():<12} (confian√ßa: {confidence:>5.1f}%)\")\n",
    "    \n",
    "    # Verificar consenso\n",
    "    predictions = [r['prediction'] for r in results.values()]\n",
    "    \n",
    "    if len(set(predictions)) == 1:\n",
    "        print(f\"\\n‚úÖ CONSENSO: Todos concordam que √© '{predictions[0]}'\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è DISCORD√ÇNCIA: Os modelos n√£o concordam!\")\n",
    "    \n",
    "    # Gr√°fico de probabilidades\n",
    "    if show_plot:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "        \n",
    "        for ax, (model_name, result) in zip(axes, results.items()):\n",
    "            proba = result['probabilities']\n",
    "            pred = result['prediction']\n",
    "            \n",
    "            colors = ['green' if labels[i] == pred else 'lightblue' for i in range(len(labels))]\n",
    "            \n",
    "            ax.bar(labels, proba * 100, color=colors)\n",
    "            ax.set_title(f\"{model_name}\\nPredi√ß√£o: {pred}\")\n",
    "            ax.set_ylabel('Probabilidade (%)')\n",
    "            ax.set_ylim(0, 100)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Adicionar valores no topo das barras\n",
    "            for i, v in enumerate(proba * 100):\n",
    "                ax.text(i, v + 2, f\"{v:.0f}%\", ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úì Fun√ß√£o test_text() pronta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Textos de Exemplo\n",
    "\n",
    "Vamos testar com alguns exemplos pr√©-definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos para cada classe\n",
    "example_texts = {\n",
    "    'joy': [\n",
    "        \"I am so happy today! Everything is going perfectly!\",\n",
    "        \"This is the best day of my life, I feel amazing!\",\n",
    "    ],\n",
    "    'sadness': [\n",
    "        \"I feel so lonely and empty inside. Nothing makes sense anymore.\",\n",
    "        \"I'm heartbroken and don't know what to do.\",\n",
    "    ],\n",
    "    'love': [\n",
    "        \"I cherish every moment we spend together. You mean everything to me.\",\n",
    "        \"My heart fills with warmth whenever I think of you.\",\n",
    "    ],\n",
    "    'anger': [\n",
    "        \"I'm so furious right now! This is completely unacceptable!\",\n",
    "        \"I can't believe they did this to me. I'm absolutely livid!\",\n",
    "    ],\n",
    "    'fear': [\n",
    "        \"I'm terrified of what might happen. My hands are shaking.\",\n",
    "        \"The darkness scares me, I feel anxious and worried.\",\n",
    "    ],\n",
    "    'surprise': [\n",
    "        \"Wow! I never expected this to happen! This is incredible!\",\n",
    "        \"I'm shocked! This came completely out of nowhere!\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Exemplos de texto carregados!\")\n",
    "print(f\"Total: {sum(len(v) for v in example_texts.values())} exemplos em {len(example_texts)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Exemplo: Joy (Alegria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text(example_texts['joy'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Exemplo: Sadness (Tristeza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text(example_texts['sadness'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Exemplo: Anger (Raiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text(example_texts['anger'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Exemplo: Fear (Medo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text(example_texts['fear'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Teste com Seu Pr√≥prio Texto!\n",
    "\n",
    "Agora √© sua vez! Digite qualquer texto em ingl√™s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digite seu texto aqui:\n",
    "my_text = \"I feel nervous about the presentation tomorrow\"\n",
    "\n",
    "test_text(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Experimente mais textos!\n",
    "\n",
    "Modifique a c√©lula acima e execute novamente com diferentes textos.\n",
    "\n",
    "**Sugest√µes de testes:**\n",
    "- Textos amb√≠guos (m√∫ltiplas emo√ß√µes)\n",
    "- Textos muito curtos vs muito longos\n",
    "- Textos com sarcasmo ou ironia\n",
    "- Textos em portugu√™s (para ver como os modelos reagem)\n",
    "- Textos neutros (sem emo√ß√£o clara)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Teste em Lote\n",
    "\n",
    "Testar m√∫ltiplos textos de uma vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_test(texts: list[str]):\n",
    "    \"\"\"\n",
    "    Testa m√∫ltiplos textos e mostra resumo.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos para testar\n",
    "    \"\"\"\n",
    "    print(f\"\\nTestando {len(texts)} textos...\\n\")\n",
    "    \n",
    "    results_df = []\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"[{i}/{len(texts)}] {text[:50]}...\")\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        pred_embedding = id2label[embedding_clf.predict([text])[0]]\n",
    "        pred_finetuned = id2label[finetuned_clf.predict([text])[0]]\n",
    "        pred_llm = labels[llm_clf.predict([text])[0]]\n",
    "        \n",
    "        # Verificar consenso\n",
    "        predictions = [pred_embedding, pred_finetuned, pred_llm]\n",
    "        consensus = len(set(predictions)) == 1\n",
    "        \n",
    "        results_df.append({\n",
    "            'Texto': text[:60] + '...' if len(text) > 60 else text,\n",
    "            'Embedding': pred_embedding,\n",
    "            'Fine-tuned': pred_finetuned,\n",
    "            'LLM': pred_llm,\n",
    "            'Consenso': '‚úì' if consensus else '‚úó'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESUMO DOS RESULTADOS\")\n",
    "    print(\"=\"*80)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    consensus_count = df['Consenso'].value_counts().get('‚úì', 0)\n",
    "    consensus_pct = consensus_count / len(df) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Consenso: {consensus_count}/{len(df)} ({consensus_pct:.1f}%)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì Fun√ß√£o batch_test() pronta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Todos os Exemplos de uma Classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar todos os exemplos de 'joy'\n",
    "batch_test(example_texts['joy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar TODOS os Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten todos os exemplos\n",
    "all_examples = [text for texts in example_texts.values() for text in texts]\n",
    "\n",
    "batch_test(all_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Casos Interessantes: Textos Amb√≠guos\n",
    "\n",
    "Vamos testar textos que podem ter m√∫ltiplas interpreta√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_texts = [\n",
    "    \"I can't believe this happened to me...\",  # Surprise ou sadness?\n",
    "    \"This is just perfect.\",  # Joy ou sarcasmo (anger)?\n",
    "    \"I'm overwhelmed by everything right now.\",  # Fear, sadness ou surprise?\n",
    "    \"My heart is racing.\",  # Fear, love ou surprise?\n",
    "    \"I don't know what to feel anymore.\",  # Sadness ou confusion?\n",
    "]\n",
    "\n",
    "print(\"Testando textos amb√≠guos...\\n\")\n",
    "print(\"Estes textos podem ter m√∫ltiplas interpreta√ß√µes.\")\n",
    "print(\"Vamos ver se os modelos concordam ou discordam.\\n\")\n",
    "\n",
    "batch_test(ambiguous_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise Detalhada de um Caso Amb√≠guo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolher um texto amb√≠guo para an√°lise detalhada\n",
    "test_text(ambiguous_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compara√ß√£o de Confian√ßa\n",
    "\n",
    "Qual modelo √© mais \"confiante\" nas suas predi√ß√µes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_confidence(texts: list[str]):\n",
    "    \"\"\"\n",
    "    Compara a confian√ßa (probabilidade m√°xima) de cada modelo.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos para testar\n",
    "    \"\"\"\n",
    "    confidences = {'Embedding': [], 'Fine-tuned': [], 'LLM': []}\n",
    "    \n",
    "    for text in texts:\n",
    "        # Predi√ß√µes com probabilidades\n",
    "        proba_embedding = embedding_clf.predict_proba([text])[0]\n",
    "        proba_finetuned = finetuned_clf.predict_proba([text])[0]\n",
    "        proba_llm = llm_clf.predict_proba([text])[0]\n",
    "        \n",
    "        # Pegar confian√ßa (max probability)\n",
    "        confidences['Embedding'].append(proba_embedding.max())\n",
    "        confidences['Fine-tuned'].append(proba_finetuned.max())\n",
    "        confidences['LLM'].append(proba_llm.max())\n",
    "    \n",
    "    # Calcular m√©dias\n",
    "    avg_confidences = {k: np.mean(v) * 100 for k, v in confidences.items()}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONFIAN√áA M√âDIA POR MODELO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for model, conf in sorted(avg_confidences.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{model:<15}: {conf:>5.1f}%\")\n",
    "    \n",
    "    # Gr√°fico\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    positions = np.arange(len(texts))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(positions - width, np.array(confidences['Embedding']) * 100, width, label='Embedding', alpha=0.8)\n",
    "    ax.bar(positions, np.array(confidences['Fine-tuned']) * 100, width, label='Fine-tuned', alpha=0.8)\n",
    "    ax.bar(positions + width, np.array(confidences['LLM']) * 100, width, label='LLM', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Texto')\n",
    "    ax.set_ylabel('Confian√ßa (%)')\n",
    "    ax.set_title('Confian√ßa dos Modelos por Texto')\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels([f\"Texto {i+1}\" for i in range(len(texts))])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Testar com textos amb√≠guos\n",
    "compare_confidence(ambiguous_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Interpreta√ß√£o da Confian√ßa\n",
    "\n",
    "**Alta confian√ßa (>80%):**\n",
    "- Modelo est√° \"certo\" da resposta\n",
    "- Texto tem sinais claros da emo√ß√£o\n",
    "\n",
    "**M√©dia confian√ßa (50-80%):**\n",
    "- Modelo tem d√∫vidas\n",
    "- Texto pode ser amb√≠guo\n",
    "\n",
    "**Baixa confian√ßa (<50%):**\n",
    "- Modelo est√° \"chutando\"\n",
    "- Texto muito amb√≠guo ou fora do dom√≠nio de treino\n",
    "\n",
    "**Modelo sempre muito confiante:**\n",
    "- Pode indicar overfitting\n",
    "- Ou modelo muito simplista\n",
    "\n",
    "**Modelo sempre pouco confiante:**\n",
    "- Pode precisar de mais treinamento\n",
    "- Ou dados de teste muito diferentes do treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lise Explorat√≥ria Livre\n",
    "\n",
    "Use esta c√©lula para seus pr√≥prios experimentos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espa√ßo para seus experimentos\n",
    "\n",
    "# Exemplo: Testar textos em portugu√™s\n",
    "portuguese_texts = [\n",
    "    \"Estou muito feliz hoje!\",\n",
    "    \"Estou triste e sozinho.\",\n",
    "    \"Eu te amo muito!\",\n",
    "]\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è AVISO: Modelos foram treinados em INGL√äS!\")\n",
    "print(\"Resultados em portugu√™s ser√£o ruins (esperado).\\n\")\n",
    "\n",
    "batch_test(portuguese_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumo e Conclus√µes\n",
    "\n",
    "### üéì O que aprendemos?\n",
    "\n",
    "1. **Consenso ‚â† Correto**: Todos concordarem n√£o garante que est√£o certos\n",
    "2. **Confian√ßa ‚â† Precis√£o**: Um modelo confiante pode estar errado\n",
    "3. **Contexto importa**: Mesma palavra pode ter emo√ß√µes diferentes\n",
    "4. **Ambiguidade √© real**: Nem sempre h√° uma resposta \"certa\"\n",
    "\n",
    "### üìä Pr√≥ximos Passos\n",
    "\n",
    "1. **Teste mais textos** do seu dom√≠nio de interesse\n",
    "2. **Identifique padr√µes**: Quando cada modelo √© melhor?\n",
    "3. **Ajuste o modelo**: Use insights para melhorar treinamento\n",
    "4. **Colete dados**: Salve casos interessantes para an√°lise\n",
    "\n",
    "### üöÄ Dicas para Produ√ß√£o\n",
    "\n",
    "- **Use ensemble**: Combine predi√ß√µes dos 3 modelos\n",
    "- **Defina threshold**: Rejeite predi√ß√µes com baixa confian√ßa\n",
    "- **Monitore**: Salve casos onde modelos discordam\n",
    "- **Feedback loop**: Use corre√ß√µes de usu√°rio para re-treinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exportar Seus Testes\n",
    "\n",
    "Salvar seus experimentos para refer√™ncia futura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Salvar resultados de batch test\n",
    "\n",
    "# my_texts = [...] # Seus textos\n",
    "# results_df = batch_test(my_texts)\n",
    "\n",
    "# Salvar em CSV\n",
    "# output_path = Path('../results/my_experiments.csv')\n",
    "# results_df.to_csv(output_path, index=False)\n",
    "# print(f\"‚úì Resultados salvos em: {output_path}\")\n",
    "\n",
    "print(\"Use as c√©lulas acima para salvar seus experimentos!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f2_n6_semantic-search-and-data-classificat-QWY73G8N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
