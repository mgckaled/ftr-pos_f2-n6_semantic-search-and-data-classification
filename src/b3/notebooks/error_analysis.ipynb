{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise de Erros dos Classificadores\n",
    "\n",
    "**Execute este notebook AP√ìS rodar `main.py`**\n",
    "\n",
    "Aqui voc√™ vai:\n",
    "- Ver onde cada modelo erra\n",
    "- Identificar padr√µes nos erros\n",
    "- Encontrar casos onde modelos simples batem modelos complexos\n",
    "- Entender quais textos s√£o dif√≠ceis de classificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregar Dados e Predi√ß√µes\n",
    "\n",
    "Se voc√™ rodou `main.py`, as predi√ß√µes foram salvas em `results/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../results')\n",
    "\n",
    "# Verificar se resultados existem\n",
    "if not results_dir.exists():\n",
    "    print(\"‚ö†Ô∏è Pasta 'results/' n√£o encontrada!\")\n",
    "    print(\"Execute 'python main.py' primeiro.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Carregar predi√ß√µes salvas (voc√™ precisar√° modificar main.py para salv√°-las)\n",
    "try:\n",
    "    with open(results_dir / 'predictions.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    X_test = data['X_test']\n",
    "    y_true = data['y_true']\n",
    "    y_pred_embedding = data['y_pred_embedding']\n",
    "    y_pred_finetuned = data['y_pred_finetuned']\n",
    "    y_pred_llm = data['y_pred_llm']\n",
    "    id2label = data['id2label']\n",
    "    \n",
    "    print(\"‚úì Predi√ß√µes carregadas com sucesso!\")\n",
    "    print(f\"\\nTotal de amostras de teste: {len(X_test)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Arquivo 'predictions.pkl' n√£o encontrado!\")\n",
    "    print(\"\\n‚ö†Ô∏è IMPORTANTE: Voc√™ precisa modificar main.py para salvar as predi√ß√µes.\")\n",
    "    print(\"\\nAdicione no final da fun√ß√£o main():\")\n",
    "    print(\"\"\"\n",
    "    # Salvar predi√ß√µes para an√°lise de erros\n",
    "    predictions_data = {\n",
    "        'X_test': X_test,\n",
    "        'y_true': y_test,\n",
    "        'y_pred_embedding': embedding_clf.predict(X_test),\n",
    "        'y_pred_finetuned': finetuned_clf.predict(X_test),\n",
    "        'y_pred_llm': llm_clf.predict(X_test),\n",
    "        'id2label': id2label\n",
    "    }\n",
    "    \n",
    "    import pickle\n",
    "    with open(config.RESULTS_DIR / 'predictions.pkl', 'wb') as f:\n",
    "        pickle.dump(predictions_data, f)\n",
    "    \"\"\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Criar DataFrame de An√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com todas as informa√ß√µes\n",
    "df_analysis = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true_label': y_true,\n",
    "    'true_label_name': [id2label[label] for label in y_true],\n",
    "    'pred_embedding': y_pred_embedding,\n",
    "    'pred_finetuned': y_pred_finetuned,\n",
    "    'pred_llm': y_pred_llm,\n",
    "    'pred_embedding_name': [id2label[label] for label in y_pred_embedding],\n",
    "    'pred_finetuned_name': [id2label[label] for label in y_pred_finetuned],\n",
    "    'pred_llm_name': [id2label[label] for label in y_pred_llm],\n",
    "})\n",
    "\n",
    "# Adicionar colunas de acerto/erro\n",
    "df_analysis['correct_embedding'] = df_analysis['true_label'] == df_analysis['pred_embedding']\n",
    "df_analysis['correct_finetuned'] = df_analysis['true_label'] == df_analysis['pred_finetuned']\n",
    "df_analysis['correct_llm'] = df_analysis['true_label'] == df_analysis['pred_llm']\n",
    "\n",
    "# Contar quantos modelos acertaram\n",
    "df_analysis['num_correct'] = (\n",
    "    df_analysis['correct_embedding'].astype(int) +\n",
    "    df_analysis['correct_finetuned'].astype(int) +\n",
    "    df_analysis['correct_llm'].astype(int)\n",
    ")\n",
    "\n",
    "print(\"DataFrame de an√°lise criado!\")\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vis√£o Geral dos Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo de acertos por modelo\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO DE ACERTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model in ['embedding', 'finetuned', 'llm']:\n",
    "    correct = df_analysis[f'correct_{model}'].sum()\n",
    "    total = len(df_analysis)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"{model.upper():<15}: {correct:>4}/{total} ({accuracy:>5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONSENSO DOS MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consensus_counts = df_analysis['num_correct'].value_counts().sort_index(ascending=False)\n",
    "\n",
    "for num_correct, count in consensus_counts.items():\n",
    "    pct = count / len(df_analysis) * 100\n",
    "    if num_correct == 3:\n",
    "        label = \"Todos acertaram\"\n",
    "    elif num_correct == 2:\n",
    "        label = \"2 modelos acertaram\"\n",
    "    elif num_correct == 1:\n",
    "        label = \"Apenas 1 acertou\"\n",
    "    else:\n",
    "        label = \"NENHUM acertou ‚ùå\"\n",
    "    \n",
    "    print(f\"{label:<25}: {count:>4} amostras ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Casos Onde NENHUM Modelo Acertou\n",
    "\n",
    "Estes s√£o os casos mais dif√≠ceis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos onde todos erraram\n",
    "all_wrong = df_analysis[df_analysis['num_correct'] == 0]\n",
    "\n",
    "print(f\"Total de casos onde TODOS erraram: {len(all_wrong)}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Mostrar alguns exemplos\n",
    "num_examples = min(10, len(all_wrong))\n",
    "\n",
    "for i, (idx, row) in enumerate(all_wrong.head(num_examples).iterrows(), 1):\n",
    "    print(f\"\\n{i}. Texto: {row['text'][:100]}...\")\n",
    "    print(f\"   ‚úì Verdadeiro: {row['true_label_name']}\")\n",
    "    print(f\"   ‚úó Embedding:  {row['pred_embedding_name']}\")\n",
    "    print(f\"   ‚úó Fine-tuned: {row['pred_finetuned_name']}\")\n",
    "    print(f\"   ‚úó LLM:        {row['pred_llm_name']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç An√°lise: Por que todos erraram?\n",
    "\n",
    "**Poss√≠veis raz√µes:**\n",
    "\n",
    "1. **Ambiguidade genu√≠na**: O texto pode expressar m√∫ltiplas emo√ß√µes\n",
    "2. **Label errado no dataset**: Pode ser erro de anota√ß√£o\n",
    "3. **Contexto insuficiente**: Falta informa√ß√£o para determinar a emo√ß√£o\n",
    "4. **Vi√©s do dataset**: Padr√µes n√£o representativos no treino\n",
    "5. **Classe minorit√°ria**: Classes raras (ex: surprise) s√£o mais dif√≠ceis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Casos Onde Apenas UM Modelo Acertou\n",
    "\n",
    "Vamos ver qual modelo √© melhor em casos dif√≠ceis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos onde apenas 1 acertou\n",
    "only_one_correct = df_analysis[df_analysis['num_correct'] == 1]\n",
    "\n",
    "print(f\"Total de casos onde APENAS 1 modelo acertou: {len(only_one_correct)}\")\n",
    "print(\"\\nQuem acertou quando os outros erraram?\\n\")\n",
    "\n",
    "# Contar vit√≥rias individuais\n",
    "embedding_alone = only_one_correct[\n",
    "    only_one_correct['correct_embedding'] & \n",
    "    ~only_one_correct['correct_finetuned'] & \n",
    "    ~only_one_correct['correct_llm']\n",
    "]\n",
    "\n",
    "finetuned_alone = only_one_correct[\n",
    "    ~only_one_correct['correct_embedding'] & \n",
    "    only_one_correct['correct_finetuned'] & \n",
    "    ~only_one_correct['correct_llm']\n",
    "]\n",
    "\n",
    "llm_alone = only_one_correct[\n",
    "    ~only_one_correct['correct_embedding'] & \n",
    "    ~only_one_correct['correct_finetuned'] & \n",
    "    only_one_correct['correct_llm']\n",
    "]\n",
    "\n",
    "print(f\"Embedding sozinho:  {len(embedding_alone):>4} casos\")\n",
    "print(f\"Fine-tuned sozinho: {len(finetuned_alone):>4} casos\")\n",
    "print(f\"LLM sozinho:        {len(llm_alone):>4} casos\")\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "labels = ['Embedding', 'Fine-tuned', 'LLM']\n",
    "sizes = [len(embedding_alone), len(finetuned_alone), len(llm_alone)]\n",
    "colors = ['#FF9999', '#66B3FF', '#99FF99']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Vit√≥rias Individuais: Quem acertou quando os outros erraram?')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos de Vit√≥rias Individuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar exemplos onde cada modelo foi o √∫nico a acertar\n",
    "datasets = [\n",
    "    ('EMBEDDING', embedding_alone),\n",
    "    ('FINE-TUNED', finetuned_alone),\n",
    "    ('LLM', llm_alone)\n",
    "]\n",
    "\n",
    "for model_name, df_subset in datasets:\n",
    "    if len(df_subset) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Casos onde APENAS {model_name} acertou\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    num_examples = min(3, len(df_subset))\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df_subset.head(num_examples).iterrows(), 1):\n",
    "        print(f\"\\n{i}. Texto: {row['text'][:100]}...\")\n",
    "        print(f\"   ‚úì Verdadeiro:  {row['true_label_name']}\")\n",
    "        print(f\"   {'‚úì' if row['correct_embedding'] else '‚úó'} Embedding:   {row['pred_embedding_name']}\")\n",
    "        print(f\"   {'‚úì' if row['correct_finetuned'] else '‚úó'} Fine-tuned:  {row['pred_finetuned_name']}\")\n",
    "        print(f\"   {'‚úì' if row['correct_llm'] else '‚úó'} LLM:         {row['pred_llm_name']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelo Simples vs Complexo\n",
    "\n",
    "Casos onde Embedding (simples) acertou mas Fine-tuned (complexo) errou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos onde embedding acertou MAS finetuned errou\n",
    "simple_beats_complex = df_analysis[\n",
    "    df_analysis['correct_embedding'] & ~df_analysis['correct_finetuned']\n",
    "]\n",
    "\n",
    "# Casos onde finetuned acertou MAS embedding errou\n",
    "complex_beats_simple = df_analysis[\n",
    "    ~df_analysis['correct_embedding'] & df_analysis['correct_finetuned']\n",
    "]\n",
    "\n",
    "print(\"Compara√ß√£o: Embedding vs Fine-tuned\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Embedding acertou, Fine-tuned errou:  {len(simple_beats_complex):>4} casos\")\n",
    "print(f\"Fine-tuned acertou, Embedding errou:  {len(complex_beats_simple):>4} casos\")\n",
    "print(f\"\\nRaz√£o (Complex/Simple):               {len(complex_beats_simple) / max(len(simple_beats_complex), 1):.2f}x\")\n",
    "\n",
    "# Mostrar exemplos onde simples bateu complexo\n",
    "if len(simple_beats_complex) > 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Exemplos: Embedding (simples) acertou mas Fine-tuned (complexo) errou\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(simple_beats_complex.head(5).iterrows(), 1):\n",
    "        print(f\"\\n{i}. Texto: {row['text'][:100]}...\")\n",
    "        print(f\"   ‚úì Verdadeiro:  {row['true_label_name']}\")\n",
    "        print(f\"   ‚úì Embedding:   {row['pred_embedding_name']} (ACERTOU)\")\n",
    "        print(f\"   ‚úó Fine-tuned:  {row['pred_finetuned_name']} (ERROU)\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Insight: Quando o modelo simples ganha?\n",
    "\n",
    "**Poss√≠veis raz√µes:**\n",
    "\n",
    "1. **Overfitting**: Fine-tuned pode ter decorado padr√µes espec√≠ficos do treino\n",
    "2. **Generaliza√ß√£o**: Embeddings pr√©-treinados t√™m conhecimento mais geral\n",
    "3. **Dataset pequeno**: Fine-tuning com poucos dados pode n√£o funcionar bem\n",
    "4. **Hiperpar√¢metros**: Learning rate, epochs podem estar mal configurados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Erros por Classe\n",
    "\n",
    "Quais classes cada modelo tem mais dificuldade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz de erros por classe\n",
    "error_by_class = []\n",
    "\n",
    "for label_id, label_name in id2label.items():\n",
    "    df_class = df_analysis[df_analysis['true_label'] == label_id]\n",
    "    \n",
    "    if len(df_class) == 0:\n",
    "        continue\n",
    "    \n",
    "    total = len(df_class)\n",
    "    \n",
    "    error_by_class.append({\n",
    "        'Classe': label_name,\n",
    "        'Total': total,\n",
    "        'Embedding Erros': (~df_class['correct_embedding']).sum(),\n",
    "        'Fine-tuned Erros': (~df_class['correct_finetuned']).sum(),\n",
    "        'LLM Erros': (~df_class['correct_llm']).sum(),\n",
    "        'Embedding Erro %': (~df_class['correct_embedding']).sum() / total * 100,\n",
    "        'Fine-tuned Erro %': (~df_class['correct_finetuned']).sum() / total * 100,\n",
    "        'LLM Erro %': (~df_class['correct_llm']).sum() / total * 100,\n",
    "    })\n",
    "\n",
    "df_errors = pd.DataFrame(error_by_class)\n",
    "\n",
    "print(\"Taxa de Erro por Classe (%)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_errors[['Classe', 'Total', 'Embedding Erro %', 'Fine-tuned Erro %', 'LLM Erro %']].to_string(index=False))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(df_errors))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, df_errors['Embedding Erro %'], width, label='Embedding', color='#FF9999')\n",
    "ax.bar(x, df_errors['Fine-tuned Erro %'], width, label='Fine-tuned', color='#66B3FF')\n",
    "ax.bar(x + width, df_errors['LLM Erro %'], width, label='LLM', color='#99FF99')\n",
    "\n",
    "ax.set_xlabel('Classe')\n",
    "ax.set_ylabel('Taxa de Erro (%)')\n",
    "ax.set_title('Taxa de Erro por Classe')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_errors['Classe'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confus√µes Mais Comuns\n",
    "\n",
    "Quais pares de classes s√£o mais confundidos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_confusions(y_true, y_pred, id2label, top_n=5):\n",
    "    \"\"\"Retorna as confus√µes mais comuns (exceto acertos)\"\"\"\n",
    "    confusions = []\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label != pred_label:  # Apenas erros\n",
    "            confusions.append((id2label[true_label], id2label[pred_label]))\n",
    "    \n",
    "    counter = Counter(confusions)\n",
    "    return counter.most_common(top_n)\n",
    "\n",
    "# Confus√µes mais comuns de cada modelo\n",
    "models = [\n",
    "    ('Embedding', y_pred_embedding),\n",
    "    ('Fine-tuned', y_pred_finetuned),\n",
    "    ('LLM', y_pred_llm)\n",
    "]\n",
    "\n",
    "for model_name, y_pred in models:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Top 5 Confus√µes: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    top_confusions = get_top_confusions(y_true, y_pred, id2label)\n",
    "    \n",
    "    for i, ((true_class, pred_class), count) in enumerate(top_confusions, 1):\n",
    "        print(f\"{i}. '{true_class}' ‚Üí confundido com '{pred_class}': {count} vezes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç An√°lise de Confus√µes\n",
    "\n",
    "**Confus√µes esperadas:**\n",
    "\n",
    "- **sadness ‚Üî fear**: Ambas s√£o emo√ß√µes negativas\n",
    "- **joy ‚Üî love**: Ambas s√£o emo√ß√µes positivas\n",
    "- **anger ‚Üî fear**: Contextos de amea√ßa\n",
    "- **surprise ‚Üî joy**: Surpresas geralmente positivas\n",
    "\n",
    "**Se vir confus√µes inesperadas:**\n",
    "- Pode indicar problema nos dados de treino\n",
    "- Ou ambiguidade genu√≠na nas labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exemplos de Confus√µes Espec√≠ficas\n",
    "\n",
    "Vamos ver textos reais das confus√µes mais comuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_examples(df, true_class, pred_class, model_col, num_examples=3):\n",
    "    \"\"\"Mostra exemplos de uma confus√£o espec√≠fica\"\"\"\n",
    "    mask = (df['true_label_name'] == true_class) & (df[model_col] == pred_class)\n",
    "    examples = df[mask].head(num_examples)\n",
    "    \n",
    "    if len(examples) == 0:\n",
    "        print(\"   (Nenhum exemplo encontrado)\")\n",
    "        return\n",
    "    \n",
    "    for i, (idx, row) in enumerate(examples.iterrows(), 1):\n",
    "        print(f\"\\n   {i}. {row['text'][:100]}...\")\n",
    "\n",
    "# Pegar confus√£o mais comum do fine-tuned\n",
    "top_confusion = get_top_confusions(y_true, y_pred_finetuned, id2label, top_n=1)[0]\n",
    "true_class, pred_class = top_confusion[0]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Exemplos: '{true_class}' confundido com '{pred_class}' (Fine-tuned)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "show_confusion_examples(df_analysis, true_class, pred_class, 'pred_finetuned_name', num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recomenda√ß√µes Baseadas nos Erros\n",
    "\n",
    "### üéØ Como Melhorar os Modelos?\n",
    "\n",
    "**Baseado na an√°lise de erros:**\n",
    "\n",
    "1. **Se muitos erros nas mesmas classes:**\n",
    "   - Coletar mais dados dessas classes\n",
    "   - Aumentar peso dessas classes no treinamento\n",
    "   - Usar data augmentation\n",
    "\n",
    "2. **Se confus√µes entre classes espec√≠ficas:**\n",
    "   - Revisar as labels do dataset (pode ter erro de anota√ß√£o)\n",
    "   - Adicionar features que diferenciem essas classes\n",
    "   - Considerar fus√£o de classes muito similares\n",
    "\n",
    "3. **Se modelo simples bate complexo:**\n",
    "   - Reduzir learning rate\n",
    "   - Aumentar regulariza√ß√£o (weight decay)\n",
    "   - Treinar por mais √©pocas\n",
    "   - Coletar mais dados de treino\n",
    "\n",
    "4. **Se todos os modelos erram nos mesmos casos:**\n",
    "   - Problema pode ser no dataset (labels ruins)\n",
    "   - Ou s√£o casos genuinamente amb√≠guos\n",
    "   - Considerar rejeitar predi√ß√µes com baixa confian√ßa\n",
    "\n",
    "### üìä Pr√≥ximos Passos\n",
    "\n",
    "1. **Analise as confus√µes** mais comuns de cada modelo\n",
    "2. **Leia os exemplos** onde todos erraram\n",
    "3. **Compare com seus objetivos**: Qual tipo de erro √© mais cr√≠tico?\n",
    "4. **Ajuste o modelo** baseado nos insights\n",
    "5. **Re-treine e compare** novamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exportar An√°lise\n",
    "\n",
    "Salvar an√°lise para refer√™ncia futura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar DataFrame completo\n",
    "output_path = results_dir / 'error_analysis.csv'\n",
    "df_analysis.to_csv(output_path, index=False)\n",
    "print(f\"‚úì An√°lise salva em: {output_path}\")\n",
    "\n",
    "# Salvar resumo de erros por classe\n",
    "error_summary_path = results_dir / 'error_summary_by_class.csv'\n",
    "df_errors.to_csv(error_summary_path, index=False)\n",
    "print(f\"‚úì Resumo de erros salvo em: {error_summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
