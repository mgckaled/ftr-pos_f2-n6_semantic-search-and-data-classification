# Guia de Interpreta√ß√£o - Resultados e Visualiza√ß√µes

**Para iniciantes**: Como entender tudo que o projeto gera

---

## √çndice

1. [Sa√≠da do Data Loader](#1-sa√≠da-do-data-loader)
2. [M√©tricas de Classifica√ß√£o](#2-m√©tricas-de-classifica√ß√£o)
3. [Matriz de Confus√£o](#3-matriz-de-confus√£o)
4. [Gr√°ficos de Compara√ß√£o](#4-gr√°ficos-de-compara√ß√£o)
5. [Curva ROC e AUC](#5-curva-roc-e-auc)
6. [An√°lise de Resultados](#6-an√°lise-de-resultados)

---

## 1. Sa√≠da do Data Loader

### O que voc√™ v√™

```
Dataset carregado com sucesso!
Treino: 16000 amostras
Teste: 2000 amostras
Classes: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
Distribui√ß√£o de classes (treino):
label
0    4666
1    5362
2    1304
3    2159
4    1937
5     572
```

### O que significa

**Amostras de treino vs teste:**
- **Treino (16000)**: Dados que o modelo vai "estudar"
- **Teste (2000)**: Dados que o modelo NUNCA viu (prova final)
- **Propor√ß√£o**: 80% treino, 20% teste (padr√£o)

**Classes:**
- S√£o as "categorias" que queremos prever
- Exemplo: dado o texto "i feel sad", queremos que o modelo diga "sadness"

**Distribui√ß√£o de classes:**

```
Classe      Quantidade    % do total    O que significa
-------------------------------------------------------
sadness     4666          29.2%         Classe comum
joy         5362          33.5%         Classe MAIS comum (maioria)
love        1304           8.2%         Classe rara
anger       2159          13.5%         Classe mediana
fear        1937          12.1%         Classe mediana
surprise     572           3.6%         Classe MUITO rara (minoria)
```

**‚ö†Ô∏è ATEN√á√ÉO - Desbalanceamento:**

Isso √© **desbalanceado**! Significa:
- Modelo pode aprender melhor "joy" (muitos exemplos)
- Modelo pode ter dificuldade com "surprise" (poucos exemplos)

**Analogia:** √â como estudar para uma prova onde:
- 33% das quest√µes s√£o de Matem√°tica (joy)
- 3% das quest√µes s√£o de F√≠sica (surprise)

Voc√™ naturalmente vai acertar mais Matem√°tica!

---

## 2. M√©tricas de Classifica√ß√£o

### O que voc√™ v√™

```
M√©tricas Gerais:
  Accuracy:  0.7500
  Precision: 0.7300
  Recall:    0.7100
  F1-Score:  0.7200
```

### O que significa (explica√ß√£o simples)

#### **Accuracy (Acur√°cia) = 0.75**

**Pergunta:** "De todas as predi√ß√µes, quantas % est√£o corretas?"

**C√°lculo:**
```
Accuracy = (Acertos) / (Total de predi√ß√µes)
         = 1500 acertos / 2000 predi√ß√µes
         = 0.75 = 75%
```

**Interpreta√ß√£o:**
- 75% = O modelo acerta 3 em cada 4 textos
- 25% = O modelo erra 1 em cada 4 textos

**√â bom ou ruim?**
- < 50%: üòû Muito ruim (chute aleat√≥rio)
- 50-70%: üòê Razo√°vel
- 70-85%: üôÇ Bom
- 85-95%: üòÄ Muito bom
- > 95%: ü§© Excelente (ou overfitting!)

---

#### **Precision (Precis√£o) = 0.73**

**Pergunta:** "Quando o modelo diz que √© classe X, ele est√° certo quantas % das vezes?"

**Exemplo pr√°tico:**
```
Modelo disse "joy" 100 vezes
  ‚Üí 73 vezes estava CERTO (era realmente joy)
  ‚Üí 27 vezes estava ERRADO (era outra emo√ß√£o)

Precision = 73/100 = 0.73 = 73%
```

**Analogia:** Teste de gravidez
- **Alta precis√£o**: Quando diz "gr√°vida", realmente est√° gr√°vida
- **Baixa precis√£o**: Muitos falsos positivos (diz gr√°vida mas n√£o est√°)

**Quando √© importante?**
- Filtro de spam (n√£o quero emails importantes indo pro spam)
- Diagn√≥stico m√©dico (n√£o quero dizer que est√° doente se n√£o est√°)

---

#### **Recall (Revoca√ß√£o) = 0.71**

**Pergunta:** "De todos os casos REAIS de classe X, quantos % o modelo conseguiu encontrar?"

**Exemplo pr√°tico:**
```
No teste, existiam 100 textos REALMENTE de "joy"
  ‚Üí Modelo encontrou 71 deles
  ‚Üí Modelo perdeu 29 (classificou como outra coisa)

Recall = 71/100 = 0.71 = 71%
```

**Analogia:** Detector de metal
- **Alto recall**: Encontra todas as moedas (mas pode dar falso alarme)
- **Baixo recall**: Perde muitas moedas

**Quando √© importante?**
- Detec√ß√£o de fraude (n√£o podemos deixar nenhuma fraude passar)
- Diagn√≥stico de c√¢ncer (n√£o podemos perder nenhum caso)

---

#### **F1-Score = 0.72**

**Pergunta:** "Qual a 'm√©dia harm√¥nica' entre Precision e Recall?"

**C√°lculo:**
```
F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
   = 2 √ó (0.73 √ó 0.71) / (0.73 + 0.71)
   = 0.72
```

**Por que √© √∫til?**
- Precision e Recall t√™m um **trade-off**
- F1 balanceia os dois
- M√©trica √∫nica mais equilibrada

**Analogia do trade-off:**

Imagine um detector de fuma√ßa:

| Configura√ß√£o | Precision | Recall | O que acontece |
|--------------|-----------|--------|----------------|
| Muito sens√≠vel | Baixa (0.50) | Alta (0.95) | Apita por qualquer fumacinha (muitos falsos alarmes) |
| Pouco sens√≠vel | Alta (0.90) | Baixa (0.50) | S√≥ apita com inc√™ndio grande (perde casos pequenos) |
| **Balanceado** | **Boa (0.75)** | **Boa (0.75)** | **F1 alto: equilibrado!** |

---

### M√©tricas por Classe

```
Classe          Precision    Recall       F1-Score
---------------------------------------------------
sadness         0.8000       0.7500       0.7700
joy             0.8500       0.9000       0.8700
love            0.5000       0.4000       0.4400
anger           0.7000       0.6500       0.6700
fear            0.6800       0.6200       0.6500
surprise        0.3500       0.2500       0.2900
```

**Interpreta√ß√£o:**

| Classe | Performance | Por qu√™? |
|--------|-------------|----------|
| **joy** | üòÄ Excelente (F1=0.87) | Muitos exemplos no treino (5362) |
| **sadness** | üôÇ Boa (F1=0.77) | Muitos exemplos (4666) |
| **anger, fear** | üòê Razo√°vel (F1~0.65) | Exemplos medianos |
| **love** | üòü Fraca (F1=0.44) | Poucos exemplos (1304) |
| **surprise** | üòû Ruim (F1=0.29) | **MUITO** poucos exemplos (572) |

**Conclus√£o:** Classes com mais dados t√™m melhor performance!

---

## 3. Matriz de Confus√£o

### O que voc√™ v√™

```
                Predito
             sad  joy  love  anger  fear  surprise
Real    sad   75   10    5     8     2      0
        joy    8   90    2     0     0      0
        love   5   15   40    15     5      0
        anger  10    5    5   65    10      5
        fear    5    2    3   10    70     10
    surprise    2    5    3    8    12     20
```

### O que significa

**Cada c√©lula mostra:** Quantas vezes o modelo confundiu X com Y

**Leitura:**
- **Diagonal (verde)**: ACERTOS ‚úÖ
- **Fora da diagonal**: ERROS ‚ùå

**Exemplo linha "sadness" (75, 10, 5, 8, 2, 0):**

```
Haviam 100 textos REALMENTE de "sadness":
  ‚Üí 75 classificados CORRETAMENTE como "sadness" ‚úì
  ‚Üí 10 classificados ERRADOS como "joy" ‚úó
  ‚Üí  5 classificados ERRADOS como "love" ‚úó
  ‚Üí  8 classificados ERRADOS como "anger" ‚úó
  ‚Üí  2 classificados ERRADOS como "fear" ‚úó
  ‚Üí  0 classificados ERRADOS como "surprise" ‚úì (boa!)
```

**Recall de sadness:**
```
Recall = 75 / (75+10+5+8+2+0) = 75/100 = 0.75 = 75%
```

---

### An√°lise de confus√µes comuns

**Confus√£o 1: sadness ‚Üî joy (10 casos)**
```
Texto: "i feel so happy i could cry"
Real: joy
Predito: sadness (por causa de "cry")
```
**Por qu√™?** Textos com palavras amb√≠guas

---

**Confus√£o 2: love ‚Üí joy (15 casos)**
```
Texto: "i love spending time with my family"
Real: love
Predito: joy
```
**Por qu√™?** Emo√ß√µes positivas s√£o similares

---

**Confus√£o 3: surprise ‚Üí misto**
```
Classe "surprise" erra para TODAS as outras
Por qu√™? MUITO poucos exemplos de treino (572)
```

---

### Matriz Normalizada (%)

```
                Predito
             sad   joy  love  anger  fear  surprise
Real    sad   75%  10%   5%    8%    2%      0%
        joy    8%  90%   2%    0%    0%      0%
        love   6%  19%  50%   19%    6%      0%
    surprise   4%  10%   6%   16%   24%     40%
```

**Como ler:**
- Cada linha soma 100%
- "De todos os textos REALMENTE de X, quantos % foram para cada classe?"

**Exemplo "surprise":**
```
De todos os textos de "surprise":
  ‚Üí Apenas 40% foram classificados corretamente
  ‚Üí 24% foram confundidos com "fear"
  ‚Üí 16% foram confundidos com "anger"
  ‚Üí Resto espalhado
```

**Interpreta√ß√£o:** Modelo est√° "perdido" com surprise!

---

## 4. Gr√°ficos de Compara√ß√£o

### Gr√°fico de Barras - M√©tricas

```
    1.0 ‚î§
        ‚îÇ     ‚ñà‚ñà           ‚ñà‚ñà
    0.8 ‚î§     ‚ñà‚ñà     ‚ñà‚ñà    ‚ñà‚ñà     ‚ñà‚ñà
        ‚îÇ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
    0.6 ‚î§ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
        ‚îÇ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
    0.4 ‚î§ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
        ‚îÇ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
    0.2 ‚î§ ‚ñà‚ñà  ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà ‚ñà‚ñà
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
          Acc  Pre  Rec  F1  Acc Pre ...
         Embedding  |  Finetuned  | LLM
```

**Como interpretar:**

| M√©trica | Embedding | Finetuned | LLM | Vencedor |
|---------|-----------|-----------|-----|----------|
| Accuracy | 0.75 | **0.88** | 0.82 | üèÜ Finetuned |
| Precision | 0.73 | **0.86** | 0.80 | üèÜ Finetuned |
| Recall | 0.72 | **0.85** | 0.79 | üèÜ Finetuned |
| F1-Score | 0.72 | **0.85** | 0.80 | üèÜ Finetuned |

**Conclus√£o:** Fine-tuned √© o melhor em TODAS as m√©tricas!

---

### Gr√°fico de Tempo de Infer√™ncia

```
    6s ‚î§                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    5s ‚î§                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    4s ‚î§                         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    3s ‚î§           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    2s ‚î§           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ ‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    1s ‚î§ ‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
       ‚îÇ ‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    0s ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
         Embed     Finetuned       LLM
```

**Como interpretar:**

| Classificador | Tempo (100 textos) | Velocidade |
|---------------|-------------------|------------|
| Embedding | 0.5s | üöÄ Muito r√°pido |
| Finetuned | 2.3s | üèÉ M√©dio |
| LLM | 5.7s | üê¢ Lento |

**Trade-off Velocidade vs Precis√£o:**

```
Embedding:  R√°pido (0.5s) mas menos preciso (75%)
Finetuned:  M√©dio (2.3s) e MUITO preciso (88%)  ‚Üê MELHOR!
LLM:        Lento (5.7s) e preciso (82%)
```

**Quando usar cada um?**

| Cen√°rio | Escolha | Por qu√™? |
|---------|---------|----------|
| Prot√≥tipo r√°pido | Embedding | R√°pido de implementar |
| Produ√ß√£o (alto volume) | Finetuned | Melhor precis√£o + velocidade OK |
| M√∫ltiplas tarefas | LLM | Flex√≠vel, n√£o precisa retreinar |
| Sem GPU | Embedding ou LLM | Finetuned precisa GPU para treinar |

---

## 5. Curva ROC e AUC

### O que √© ROC?

**ROC** = Receiver Operating Characteristic

√â um gr√°fico que mostra:
- **Eixo X**: Taxa de Falsos Positivos (FPR)
- **Eixo Y**: Taxa de Verdadeiros Positivos (TPR = Recall)

```
    1.0 ‚î§         ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        ‚îÇ       ‚ï±
    0.8 ‚î§     ‚ï±   ‚Üê Modelo BOM
        ‚îÇ   ‚ï±
    0.6 ‚î§  ‚ï±
        ‚îÇ ‚ï±
    0.4 ‚î§‚ï±      ‚Üê Modelo ALEAT√ìRIO (diagonal)
        ‚ï±
    0.2 ‚î§
        ‚îÇ
    0.0 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
       0.0  0.2  0.4  0.6  0.8  1.0
          Taxa Falsos Positivos
```

**Como ler:**
- Curva **colada no canto superior esquerdo** = Modelo perfeito
- Curva **na diagonal** = Modelo aleat√≥rio (chute)
- Curva **abaixo da diagonal** = Modelo pior que chute!

---

### O que √© AUC?

**AUC** = Area Under Curve (√Årea sob a curva)

√â um **n√∫mero de 0 a 1** que resume a curva ROC:

```
AUC = 0.50  üòû Modelo aleat√≥rio (chute)
AUC = 0.70  üòê Modelo razo√°vel
AUC = 0.80  üôÇ Modelo bom
AUC = 0.90  üòÄ Modelo muito bom
AUC = 0.95+ ü§© Modelo excelente
```

**Interpreta√ß√£o pr√°tica:**

```
AUC = 0.85 significa:
"Se eu pegar um exemplo positivo aleat√≥rio e um negativo aleat√≥rio,
h√° 85% de chance do modelo dar score MAIOR para o positivo"
```

**Exemplo:**

```
Texto 1: "i feel so happy today" (joy)     ‚Üí Score: 0.92
Texto 2: "i feel terrible"     (sadness)   ‚Üí Score: 0.15

O modelo deu score MAIOR para "joy" ‚úì
AUC alto significa que isso acontece consistentemente!
```

---

## 6. An√°lise de Resultados

### Exemplo Completo de Sa√≠da

```bash
=====================================
 RESULTADOS FINAIS
=====================================

[EMBEDDING + KNN]
  Accuracy:  0.7543
  Precision: 0.7312
  Recall:    0.7198
  F1-Score:  0.7254
  ROC-AUC:   0.8823
  Tempo:     0.5s

[FINE-TUNED DISTILBERT]
  Accuracy:  0.8812
  Precision: 0.8634
  Recall:    0.8521
  F1-Score:  0.8577
  ROC-AUC:   0.9543
  Tempo:     2.3s

[LLM (GEMINI)]
  Accuracy:  0.8234
  Precision: 0.8012
  Recall:    0.7923
  F1-Score:  0.7967
  ROC-AUC:   N/A
  Tempo:     5.7s
```

---

### Como Analisar

#### 1. **Qual √© o melhor modelo?**

```
Depende do crit√©rio:

Precis√£o:     Fine-tuned (88.1%)  üèÜ
Velocidade:   Embedding (0.5s)    üèÜ
Balanceado:   Fine-tuned          üèÜ
Flexibilidade: LLM                üèÜ
```

**Recomenda√ß√£o geral:** Fine-tuned DistilBERT

---

#### 2. **Por que Embedding tem AUC alto (0.88) mas Accuracy baixa (0.75)?**

**Resposta:**
- **AUC** mede capacidade de **ranquear** (separar classes)
- **Accuracy** mede acertos absolutos

**Analogia:**
```
Professor dando notas:

AUC alto = Consegue ordenar alunos do melhor ao pior
Accuracy baixa = Mas erra as notas exatas

Exemplo:
  Aluno A: Nota real 8.0 ‚Üí Deu 7.5 (ordenou certo, mas nota errada)
  Aluno B: Nota real 6.0 ‚Üí Deu 5.5 (ordenou certo, mas nota errada)
  Aluno C: Nota real 4.0 ‚Üí Deu 3.5 (ordenou certo, mas nota errada)

Ordem correta (A > B > C) ‚úì = AUC alto
Notas exatas ‚úó = Accuracy baixa
```

---

#### 3. **LLM √© melhor que Embedding, mas mais lento. Vale a pena?**

**An√°lise de custo-benef√≠cio:**

```
Embedding ‚Üí LLM:
  + Ganho de accuracy: 0.75 ‚Üí 0.82 (+7 pontos)
  - Custo de tempo: 0.5s ‚Üí 5.7s (11x mais lento)
  - Custo financeiro: $0 ‚Üí $X por chamada API

Vale a pena?
  ‚úì Se precis√£o √© cr√≠tica (ex: diagn√≥stico m√©dico)
  ‚úó Se velocidade √© cr√≠tica (ex: filtro de spam em tempo real)
```

---

#### 4. **Por que Fine-tuned √© o melhor?**

**Resposta:**

```
Fine-tuned combina:
  ‚úì Conhecimento PR√â-TREINADO do DistilBERT (ingl√™s geral)
  ‚úì ESPECIALIZA√á√ÉO nos dados espec√≠ficos (emo√ß√µes)
  ‚úì Modelo compacto e r√°pido (DistilBERT vs BERT completo)

√â como:
  Embedding = M√©dico generalista
  Fine-tuned = M√©dico generalista + ESPECIALIZA√á√ÉO em cardiologia
  LLM = Consultor m√©dico geral (sabe muito, mas gen√©rico)
```

---

### Diagn√≥stico de Problemas

#### **Problema: Accuracy muito baixa (<60%)**

**Poss√≠veis causas:**

1. **Dataset muito pequeno**
   ```
   Solu√ß√£o: Aumentar MAX_SAMPLES
   ```

2. **Poucas √©pocas de treino**
   ```
   Solu√ß√£o: Aumentar EPOCHS (2-5)
   ```

3. **Classes muito desbalanceadas**
   ```
   Solu√ß√£o: Usar weighted metrics ou balancear dados
   ```

4. **Dados ruidosos/ruins**
   ```
   Solu√ß√£o: Limpar dados, remover duplicatas
   ```

---

#### **Problema: Modelo bom no treino, ruim no teste**

**Diagn√≥stico:** **Overfitting** (decorou ao inv√©s de aprender)

```
Sinais de overfitting:
  Treino: 95% accuracy ‚úì
  Teste:  60% accuracy ‚úó

Modelo decorou padr√µes espec√≠ficos do treino
que n√£o generalizam para dados novos!
```

**Solu√ß√µes:**
1. Mais dados de treino
2. Regulariza√ß√£o (weight_decay maior)
3. Menos √©pocas
4. Data augmentation

---

#### **Problema: Modelo erra sempre a mesma classe**

```
Exemplo:
  Classe "surprise" sempre erra

Matriz de confus√£o:
  surprise: [2, 5, 3, 8, 12, 20]  ‚Üê Apenas 20/50 certos (40%)
```

**Diagn√≥stico:** Classe minorit√°ria (poucos exemplos)

**Solu√ß√µes:**
1. **Coletar mais dados** dessa classe
2. **Oversampling**: Duplicar exemplos da classe rara
3. **Undersampling**: Reduzir exemplos das classes comuns
4. **Class weights**: Dar mais "peso" √† classe rara no treinamento

---

## Resumo - Checklist de An√°lise

Ao analisar resultados, verifique:

- [ ] **Accuracy geral** > 70%?
- [ ] **F1-Score** balanceado entre classes?
- [ ] **Matriz de confus√£o** sem confus√µes bizarras?
- [ ] **Melhor modelo** tem boa rela√ß√£o precis√£o/velocidade?
- [ ] **Classes minorit√°rias** n√£o est√£o sendo ignoradas?
- [ ] **Tempo de infer√™ncia** aceit√°vel para o caso de uso?
- [ ] **Modelo generaliza** (teste similar ao treino)?

---

## Gloss√°rio R√°pido

| Termo | Significado Simples |
|-------|-------------------|
| **Accuracy** | % de acertos totais |
| **Precision** | "Quando diz X, est√° certo?" |
| **Recall** | "Encontra todos os X?" |
| **F1-Score** | Equil√≠brio entre precision e recall |
| **Overfitting** | Decorar ao inv√©s de aprender |
| **Underfitting** | N√£o aprender o suficiente |
| **Baseline** | Modelo simples para compara√ß√£o |
| **Inference** | Fazer predi√ß√µes (usar o modelo) |
| **Fine-tuning** | Especializar modelo pr√©-treinado |
| **Embeddings** | Representa√ß√£o num√©rica de texto |

---

## Para Saber Mais

Conceitos para estudar depois:

1. **Cross-validation**: Validar modelo de forma mais robusta
2. **Ensemble methods**: Combinar m√∫ltiplos modelos
3. **Hyperparameter tuning**: Otimizar par√¢metros
4. **Feature engineering**: Criar features melhores
5. **Error analysis**: Analisar profundamente os erros
