"""
Example 2: Word Embeddings with Neural Networks
================================================

Concepts covered:
- Embeddings generated by neural networks (transformers)
- Dense and semantic representations
- Comparison with one-hot encoding
- Sentence-transformers models from HuggingFace

Reference: Lesson 6 of Block A - "Generating embeddings with neural networks"

This example uses the all-MiniLM-L6-v2 model, a small (~80MB)
and efficient model for generating high-quality sentence embeddings.
"""

import json
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer


def load_texts():
    """Loads example texts from JSON file."""
    path = Path(__file__).parent.parent / "dados" / "textos_exemplo.json"
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['categories']


def analyze_embedding(embedding, word):
    """Analyzes characteristics of an embedding."""
    print(f"Analysis of embedding for '{word}':")
    print(f"  - Dimensions: {len(embedding)}")
    print(f"  - Type: dense vector (all values != 0)")
    print(f"  - Value range: [{embedding.min():.4f}, {embedding.max():.4f}]")
    print(f"  - Mean: {embedding.mean():.4f}")
    print(f"  - Standard deviation: {embedding.std():.4f}")
    print(f"  - L2 Norm: {np.linalg.norm(embedding):.4f}")
    print(f"  - First 10 values: {embedding[:10]}")
    print()


def main():
    print("=" * 70)
    print("EXAMPLE 2: WORD EMBEDDINGS WITH NEURAL NETWORKS")
    print("=" * 70)
    print()

    # ========================================================================
    # PART 1: Load Pre-trained Model
    # ========================================================================
    print("PART 1: Loading Pre-trained Model")
    print("-" * 70)
    print()

    print("Loading model: all-MiniLM-L6-v2")
    print("  - Architecture: Transformer (MiniLM)")
    print("  - Size: ~80MB")
    print("  - Output dimensions: 384")
    print("  - Training: 1B+ sentence pairs")
    print()

    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    print("[OK] Model loaded successfully!")
    print()

    # ========================================================================
    # PART 2: Generate Word/Sentence Embeddings
    # ========================================================================
    print("PART 2: Generating Embeddings")
    print("-" * 70)
    print()

    # Simple phrases to test
    sentences = [
        "cat",
        "dog",
        "table",
        "The cat is sleeping",
        "The dog is playing",
        "The table is made of wood"
    ]

    print("Generating embeddings for sentences:")
    for sentence in sentences:
        print(f"  - \"{sentence}\"")
    print()

    embeddings = model.encode(sentences)
    print(f"[OK] Generated {len(embeddings)} embeddings")
    print(f"  Shape: {embeddings.shape} (n_sentences, n_dimensions)")
    print()

    # Analyze embedding of one word
    cat_idx = 0
    analyze_embedding(embeddings[cat_idx], sentences[cat_idx])

    # ========================================================================
    # PART 3: Dense Embeddings vs Sparse One-Hot
    # ========================================================================
    print("PART 3: Dense Embeddings vs Sparse One-Hot")
    print("-" * 70)
    print()

    # Simulate one-hot for 10,000 words (typical vocabulary)
    vocab_size = 10000
    embedding_dim = 384

    print(f"Storage comparison:")
    print()
    print(f"ONE-HOT ENCODING (vocabulary = {vocab_size:,}):")
    print(f"  - Dimensions per word: {vocab_size:,}")
    print(f"  - Non-zero values: 1")
    print(f"  - Sparsity: {((vocab_size-1)/vocab_size)*100:.2f}%")
    print(f"  - Memory (float32): {vocab_size * 4 / 1024:.1f} KB per word")
    print()
    print(f"DENSE EMBEDDINGS (neural model):")
    print(f"  - Dimensions per word: {embedding_dim}")
    print(f"  - Non-zero values: {embedding_dim} (all!)")
    print(f"  - Sparsity: 0%")
    print(f"  - Memory (float32): {embedding_dim * 4 / 1024:.1f} KB per word")
    print()
    print(f"[!] Dimensionality reduction: {vocab_size / embedding_dim:.1f}x smaller!")
    print(f"   But with MUCH more semantic information!")
    print()

    # ========================================================================
    # PART 4: Embeddings Capture Semantics
    # ========================================================================
    print("PART 4: Embeddings Capture Semantic Relationships")
    print("-" * 70)
    print()

    # Load categorized texts
    categories = load_texts()

    # Select some texts from each category
    test_texts = []
    labels = []
    for category, texts in categories.items():
        test_texts.extend(texts[:2])  # First 2 from each
        labels.extend([category] * 2)

    print(f"Generating embeddings for {len(test_texts)} texts from 5 categories:")
    for i, (text, label) in enumerate(zip(test_texts, labels)):
        print(f"  [{label:12s}] {text[:50]}...")

    print()
    category_embeddings = model.encode(test_texts)
    print(f"[OK] Embeddings generated: shape {category_embeddings.shape}")
    print()

    # Calculate similarity between texts from same vs different categories
    from sklearn.metrics.pairwise import cosine_similarity

    sim_matrix = cosine_similarity(category_embeddings)

    print("Similarities (first examples):")
    print()

    # Compare animal texts (indices 0, 1)
    print(f"ANIMALS texts:")
    print(f"  Text 0: \"{test_texts[0][:40]}...\"")
    print(f"  Text 1: \"{test_texts[1][:40]}...\"")
    print(f"  Similarity: {sim_matrix[0, 1]:.4f}")
    print()

    # Compare animals with technology (indices 0, 2)
    print(f"ANIMALS vs TECHNOLOGY:")
    print(f"  Text 0 (animals): \"{test_texts[0][:40]}...\"")
    print(f"  Text 2 (technology): \"{test_texts[2][:40]}...\"")
    print(f"  Similarity: {sim_matrix[0, 2]:.4f}")
    print()

    # Calculate average intra vs inter category similarity
    n = 2  # texts per category
    n_categories = 5

    sim_intra = []  # Similarity within same category
    sim_inter = []  # Similarity between different categories

    for i in range(n_categories):
        idx_start = i * n
        # Intra-category
        for j in range(n):
            for k in range(j+1, n):
                sim_intra.append(sim_matrix[idx_start + j, idx_start + k])

        # Inter-category
        for j in range(i+1, n_categories):
            idx_other = j * n
            for a in range(n):
                for b in range(n):
                    sim_inter.append(sim_matrix[idx_start + a, idx_other + b])

    print("Similarity Statistics:")
    print(f"  Same category (intra): {np.mean(sim_intra):.4f} +/- {np.std(sim_intra):.4f}")
    print(f"  Different categories (inter): {np.mean(sim_inter):.4f} +/- {np.std(sim_inter):.4f}")
    print()
    print("[OK] Neural embeddings CAPTURE semantics:")
    print("  Similar texts have close embeddings!")
    print()

    # ========================================================================
    # PART 5: Contextual Embeddings
    # ========================================================================
    print("PART 5: Contextual Embeddings")
    print("-" * 70)
    print()

    # Polysemous word "bank" in different contexts
    context_sentences = [
        "I went to the bank to deposit money",
        "The river bank was covered with flowers"
    ]

    print("Testing polysemous word: 'bank'")
    print()
    for sentence in context_sentences:
        print(f"  \"{sentence}\"")
    print()

    context_embeddings = model.encode(context_sentences)
    sim_context = cosine_similarity([context_embeddings[0]], [context_embeddings[1]])[0][0]

    print(f"Similarity between different contexts: {sim_context:.4f}")
    print()
    print("[!] Modern models (like this one) generate CONTEXTUAL embeddings:")
    print("   The same word has different embeddings in different contexts!")
    print()

    # ========================================================================
    # CONCLUSION
    # ========================================================================
    print("=" * 70)
    print("CONCLUSIONS")
    print("=" * 70)
    print()
    print("Advantages of Neural Embeddings:")
    print()
    print("1. DENSE REPRESENTATIONS:")
    print("   [OK] Only 384 dimensions (vs 10,000+ from one-hot)")
    print("   [OK] All dimensions contain semantic information")
    print("   [OK] Efficient in memory and computation")
    print()
    print("2. CAPTURE SEMANTICS:")
    print("   [OK] Similar words/sentences -> close vectors")
    print("   [OK] Learn complex relationships from data")
    print("   [OK] Work with synonyms, paraphrases, etc")
    print()
    print("3. CONTEXTUAL:")
    print("   [OK] Same word has different embedding per context")
    print("   [OK] Resolve ambiguities automatically")
    print()
    print("4. PRE-TRAINED:")
    print("   [OK] No need to train from scratch")
    print("   [OK] Transfer learning from billions of examples")
    print("   [OK] Work well for diverse tasks")
    print()
    print("Next example: Similarity metrics between embeddings")
    print("=" * 70)


if __name__ == "__main__":
    main()
